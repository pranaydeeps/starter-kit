{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nCreated: January 2026\\nAuthor: Thomas Moerman\\nDescription: Notebook for evaluating Machine Translation outputs with standard metrics.\\n'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created: January 2026\n",
        "Author: Thomas Moerman\n",
        "Description: Notebook for evaluating Machine Translation outputs with standard metrics.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Machine Translation Evaluation\n",
        "\n",
        "This notebook demonstrates how to evaluate machine translation outputs using standard metrics.\n",
        "\n",
        "## Metrics Overview\n",
        "\n",
        "| Metric | Type | Range | Description |\n",
        "|--------|------|-------|-------------|\n",
        "| **BLEU** | N-gram overlap | 0-100 | Bilingual Evaluation Understudy - measures n-gram precision |\n",
        "| **chrF++** | Character-level | 0-100 | Character n-gram F-score with word n-grams |\n",
        "| **TER** | Edit distance | 0-‚àû | Translation Edit Rate - lower is better |\n",
        "| **COMET** | Neural | -1 to 1 | Learned metric correlating with human judgments |\n",
        "\n",
        "## When to Use Which Metric?\n",
        "\n",
        "- **BLEU**: Standard metric, good for comparing systems. Limited for morphologically rich languages.\n",
        "- **chrF++**: Better for morphologically rich languages (German, Finnish, etc.)\n",
        "- **TER**: Useful for post-editing scenarios, measures editing effort.\n",
        "- **COMET**: Best correlation with human judgment, requires GPU for efficiency.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Setup and Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking installations...\n",
            "‚úì sacrebleu 2.6.0\n",
            "‚úì COMET installed\n",
            "‚úì pandas 2.3.3\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "# Uncomment the following line if running in Colab or fresh environment\n",
        "# !pip install sacrebleu unbabel-comet pandas -q\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"Checking installations...\")\n",
        "\n",
        "try:\n",
        "    import sacrebleu\n",
        "    print(f\"‚úì sacrebleu {sacrebleu.__version__}\")\n",
        "except ImportError:\n",
        "    print(\"‚úó sacrebleu not installed. Run: pip install sacrebleu\")\n",
        "\n",
        "try:\n",
        "    import comet\n",
        "    print(f\"‚úì COMET installed\")\n",
        "except ImportError:\n",
        "    print(\"‚úó COMET not installed (optional). Run: pip install unbabel-comet\")\n",
        "\n",
        "import pandas as pd\n",
        "print(f\"‚úì pandas {pd.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Load or Create Example Data\n",
        "\n",
        "For evaluation, you need:\n",
        "- **Source sentences** (original language)\n",
        "- **Reference translations** (human/gold translations)\n",
        "- **System translations** (your MT output)\n",
        "\n",
        "Below we create example data for English‚ÜíFrench translation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 10 sentence pairs for evaluation.\n",
            "\n",
            "--- Sample ---\n",
            "Source:     The weather is beautiful today.\n",
            "Reference:  Le temps est magnifique aujourd'hui.\n",
            "System:     Le temps est beau aujourd'hui.\n"
          ]
        }
      ],
      "source": [
        "# Example data: English to French translation\n",
        "# In practice, load these from your test files\n",
        "\n",
        "source_sentences = [\n",
        "    \"The weather is beautiful today.\",\n",
        "    \"I love learning new languages.\",\n",
        "    \"Machine translation has improved significantly.\",\n",
        "    \"Can you help me find the train station?\",\n",
        "    \"The European Parliament met in Brussels yesterday.\",\n",
        "    \"Climate change is a global challenge.\",\n",
        "    \"She reads a book every week.\",\n",
        "    \"The restaurant serves excellent French cuisine.\",\n",
        "    \"We need to finish this project by Friday.\",\n",
        "    \"The concert was absolutely amazing.\",\n",
        "]\n",
        "\n",
        "# Human reference translations (gold standard)\n",
        "reference_translations = [\n",
        "    \"Le temps est magnifique aujourd'hui.\",\n",
        "    \"J'adore apprendre de nouvelles langues.\",\n",
        "    \"La traduction automatique s'est consid√©rablement am√©lior√©e.\",\n",
        "    \"Pouvez-vous m'aider √† trouver la gare?\",\n",
        "    \"Le Parlement europ√©en s'est r√©uni √† Bruxelles hier.\",\n",
        "    \"Le changement climatique est un d√©fi mondial.\",\n",
        "    \"Elle lit un livre chaque semaine.\",\n",
        "    \"Le restaurant sert une excellente cuisine fran√ßaise.\",\n",
        "    \"Nous devons terminer ce projet d'ici vendredi.\",\n",
        "    \"Le concert √©tait absolument incroyable.\",\n",
        "]\n",
        "\n",
        "# System translations (simulated MT output with varying quality)\n",
        "system_translations = [\n",
        "    \"Le temps est beau aujourd'hui.\",                              # Good, slight variation\n",
        "    \"J'aime apprendre de nouvelles langues.\",                       # Good, synonym used\n",
        "    \"La traduction automatique s'est beaucoup am√©lior√©e.\",          # Good, different adverb\n",
        "    \"Pouvez-vous m'aider √† trouver la gare?\",                       # Perfect match\n",
        "    \"Le Parlement europ√©en a rencontr√© √† Bruxelles hier.\",          # Error: wrong verb\n",
        "    \"Le changement climatique est un challenge global.\",            # Anglicism used\n",
        "    \"Elle lit un livre chaque semaine.\",                            # Perfect match\n",
        "    \"Le restaurant sert de l'excellente cuisine fran√ßaise.\",        # Minor grammar issue\n",
        "    \"Nous devons finir ce projet avant vendredi.\",                  # Synonym + preposition\n",
        "    \"Le concert √©tait vraiment incroyable.\",                        # Good, different adverb\n",
        "]\n",
        "\n",
        "print(f\"Loaded {len(source_sentences)} sentence pairs for evaluation.\")\n",
        "print(\"\\n--- Sample ---\")\n",
        "print(f\"Source:     {source_sentences[0]}\")\n",
        "print(f\"Reference:  {reference_translations[0]}\")\n",
        "print(f\"System:     {system_translations[0]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Alternative: Load from Files\n",
        "\n",
        "If you have your data in files, use this cell instead:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Uncomment to load from files\n",
        "\n",
        "# def load_sentences(file_path):\n",
        "#     \"\"\"Load sentences from a text file (one sentence per line).\"\"\"\n",
        "#     with open(file_path, 'r', encoding='utf-8') as f:\n",
        "#         return [line.strip() for line in f if line.strip()]\n",
        "# \n",
        "# # Load your data\n",
        "# source_sentences = load_sentences('data/test.en')\n",
        "# reference_translations = load_sentences('data/test.fr')\n",
        "# system_translations = load_sentences('output/predictions.txt')\n",
        "# \n",
        "# print(f\"Loaded {len(source_sentences)} sentences from files.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Calculate BLEU, chrF++, and TER\n",
        "\n",
        "Using **sacrebleu** - the standard tool for MT evaluation.\n",
        "\n",
        "### About sacrebleu\n",
        "- Provides reproducible, shareable scores\n",
        "- Handles tokenization automatically\n",
        "- Supports multiple metrics in one package\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "SACREBLEU EVALUATION RESULTS\n",
            "============================================================\n",
            "\n",
            "üìä BLEU: 54.84\n",
            "\n",
            "üìä chrF++: 76.3\n",
            "\n",
            "üìä TER: 19.35\n",
            "\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "import sacrebleu\n",
        "\n",
        "# Prepare references (sacrebleu expects a list of reference lists for multi-reference)\n",
        "references = reference_translations\n",
        "translations = system_translations\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"SACREBLEU EVALUATION RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ============ BLEU ============\n",
        "# BLEU measures n-gram precision with brevity penalty\n",
        "# Higher is better (0-100 scale)\n",
        "bleu = sacrebleu.corpus_bleu(translations, [references])\n",
        "bleu_score = round(bleu.score, 2)\n",
        "print(f\"\\nüìä BLEU: {bleu_score}\")\n",
        "\n",
        "# ============ chrF++ ============\n",
        "# chrF++ uses character n-grams + word n-grams (word_order=2)\n",
        "# Higher is better (0-100 scale)\n",
        "# Better for morphologically rich languages\n",
        "chrf = sacrebleu.corpus_chrf(translations, [references], word_order=2)\n",
        "chrf_score = round(chrf.score, 2)\n",
        "print(f\"\\nüìä chrF++: {chrf_score}\")\n",
        "\n",
        "# ============ TER ============\n",
        "# Translation Edit Rate - measures edit distance\n",
        "# LOWER is better (can exceed 100 for poor translations)\n",
        "ter = sacrebleu.corpus_ter(translations, [references])\n",
        "ter_score = round(ter.score, 2)\n",
        "print(f\"\\nüìä TER: {ter_score}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Calculate COMET Score\n",
        "\n",
        "**COMET** (Crosslingual Optimized Metric for Evaluation of Translation) is a neural metric that:\n",
        "- Uses multilingual embeddings\n",
        "- Correlates better with human judgments than BLEU\n",
        "- Requires source sentences (not just reference and hypothesis)\n",
        "\n",
        "‚ö†Ô∏è **Note**: COMET requires downloading a model (~1.5GB) and benefits from GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "COMET EVALUATION\n",
            "============================================================\n",
            "\n",
            "Downloading COMET model (this may take a while on first run)...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ea0907fba64247b8b392f14cfa16885e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5bbfe1b5c4bd49358179ad163414a1db",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ca3d47ff99254650b9ddab83daeab847",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "hparams.yaml:   0%|          | 0.00/567 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "99a993747540474cb7eda0b2c938a45c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              ".gitattributes: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "66bebd5518624ae7a8522af7ca7a7dc9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "LICENSE: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "59f2f467796b4b449ac4e9e837457098",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "checkpoints/model.ckpt:   0%|          | 0.00/2.32G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Lightning automatically upgraded your loaded checkpoint from v1.8.3.post1 to v2.6.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../.cache/huggingface/hub/models--Unbabel--wmt22-comet-da/snapshots/2760a223ac957f30acfb18c8aa649b01cf1d75f2/checkpoints/model.ckpt`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "214d19f097934039bb84cf14d44cc6e3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3913c3786872429b937a48edbbe85156",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3aaf0cc431454a41b86538846f8de09f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f80ad3d52c1245daa58904053c069a94",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/616 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Encoder model frozen.\n",
            "üí° Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "GPU available: True (cuda), used: False\n",
            "TPU available: False, using: 0 TPU cores\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing COMET scores...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Predicting DataLoader 0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2/2 [00:00<00:00,  2.45it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä COMET: 93.58\n",
            "   (Raw score: 0.9358)\n",
            "\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# COMET evaluation\n",
        "comet_score = None\n",
        "\n",
        "try:\n",
        "    from comet import download_model, load_from_checkpoint\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(\"COMET EVALUATION\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    # Prepare data for COMET (requires source, MT output, and reference)\n",
        "    comet_data = [\n",
        "        {\"src\": src, \"mt\": mt, \"ref\": ref}\n",
        "        for src, mt, ref in zip(source_sentences, system_translations, reference_translations)\n",
        "    ]\n",
        "    \n",
        "    # Download model (only needed once)\n",
        "    print(\"\\nDownloading COMET model (this may take a while on first run)...\")\n",
        "    model_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
        "    \n",
        "    # Load model\n",
        "    print(\"Loading model...\")\n",
        "    model = load_from_checkpoint(model_path)\n",
        "    \n",
        "    # Run prediction\n",
        "    print(\"Computing COMET scores...\")\n",
        "    output = model.predict(comet_data, batch_size=8, gpus=0)  # gpus=1 if you have GPU\n",
        "    \n",
        "    # Extract scores\n",
        "    segment_scores = output.scores\n",
        "    system_score = output.system_score\n",
        "    \n",
        "    comet_score = round(system_score * 100, 2)\n",
        "    print(f\"\\nüìä COMET: {comet_score}\")\n",
        "    print(f\"   (Raw score: {system_score:.4f})\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"‚ö†Ô∏è COMET not installed.\")\n",
        "    print(\"   To install: pip install unbabel-comet\")\n",
        "    print(\"   COMET provides better correlation with human judgment.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è COMET evaluation failed: {e}\")\n",
        "    print(\"   This might be due to missing dependencies or GPU issues.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Summary Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "EVALUATION SUMMARY\n",
            "============================================================\n",
            "\n",
            "\n",
            "Metric  Score          Direction\n",
            "  BLEU  54.84 ‚Üë Higher is better\n",
            "chrF++  76.30 ‚Üë Higher is better\n",
            "   TER  19.35  ‚Üì Lower is better\n",
            " COMET  93.58 ‚Üë Higher is better\n",
            "\n",
            "============================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Metric</th>\n",
              "      <th>Score</th>\n",
              "      <th>Direction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>BLEU</td>\n",
              "      <td>54.84</td>\n",
              "      <td>‚Üë Higher is better</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>chrF++</td>\n",
              "      <td>76.30</td>\n",
              "      <td>‚Üë Higher is better</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TER</td>\n",
              "      <td>19.35</td>\n",
              "      <td>‚Üì Lower is better</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>COMET</td>\n",
              "      <td>93.58</td>\n",
              "      <td>‚Üë Higher is better</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Metric  Score           Direction\n",
              "0    BLEU  54.84  ‚Üë Higher is better\n",
              "1  chrF++  76.30  ‚Üë Higher is better\n",
              "2     TER  19.35   ‚Üì Lower is better\n",
              "3   COMET  93.58  ‚Üë Higher is better"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create summary dataframe\n",
        "print(\"=\" * 60)\n",
        "print(\"EVALUATION SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "results = {\n",
        "    \"Metric\": [\"BLEU\", \"chrF++\", \"TER\"],\n",
        "    \"Score\": [bleu_score, chrf_score, ter_score],\n",
        "    \"Direction\": [\"‚Üë Higher is better\", \"‚Üë Higher is better\", \"‚Üì Lower is better\"],\n",
        "}\n",
        "\n",
        "# Add COMET if available\n",
        "if comet_score is not None:\n",
        "    results[\"Metric\"].append(\"COMET\")\n",
        "    results[\"Score\"].append(comet_score)\n",
        "    results[\"Direction\"].append(\"‚Üë Higher is better\")\n",
        "\n",
        "df_summary = pd.DataFrame(results)\n",
        "print(\"\\n\")\n",
        "print(df_summary.to_string(index=False))\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "\n",
        "# Display as a nice table\n",
        "df_summary\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Command-Line Usage\n",
        "\n",
        "You can also run sacrebleu from the command line:\n",
        "\n",
        "```bash\n",
        "# BLEU from command line:\n",
        "sacrebleu reference.txt < hypothesis.txt\n",
        "\n",
        "# chrF++:\n",
        "sacrebleu reference.txt -i hypothesis.txt -m chrf --chrf-word-order 2\n",
        "\n",
        "# TER:\n",
        "sacrebleu reference.txt -i hypothesis.txt -m ter\n",
        "\n",
        "# All metrics at once:\n",
        "sacrebleu reference.txt -i hypothesis.txt -m bleu chrf ter\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mt_starter_kit_venv (3.12.3)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
