{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nCreated: January 2026\\nAuthor: Thomas Moerman\\nDescription: Notebook for training an encoder-decoder NMT model using OpenNMT-py.\\n'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created: January 2026\n",
        "Author: Thomas Moerman\n",
        "Description: Notebook for training an encoder-decoder NMT model using OpenNMT-py.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Training an Encoder-Decoder NMT Model with OpenNMT-py\n",
        "\n",
        "This notebook walks you through **training a Transformer-based Neural Machine Translation (NMT) model** using [OpenNMT-py](https://github.com/OpenNMT/OpenNMT-py).\n",
        "\n",
        "## What you'll learn\n",
        "- How to download and prepare parallel translation data\n",
        "- How to create OpenNMT-py configuration files\n",
        "- How to build vocabularies for source and target languages\n",
        "- How to train an encoder-decoder Transformer model\n",
        "- How to translate with the trained model and evaluate with BLEU\n",
        "\n",
        "## Encoder-Decoder vs. Decoder-Only\n",
        "\n",
        "Unlike LLMs (decoder-only), traditional NMT uses an **encoder-decoder** architecture:\n",
        "- **Encoder**: Reads the source sentence and creates a representation\n",
        "- **Decoder**: Generates the target sentence based on the encoder's output\n",
        "\n",
        "This architecture is specifically designed for sequence-to-sequence tasks like translation.\n",
        "\n",
        "## OpenNMT-py\n",
        "\n",
        "OpenNMT-py is a mature, research-friendly NMT framework that supports:\n",
        "- Transformer and RNN architectures\n",
        "- Various attention mechanisms\n",
        "- Subword tokenization (BPE, SentencePiece)\n",
        "- Multi-GPU training\n",
        "- Model ensembling\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Setup and Imports\n",
        "\n",
        "First, let's verify that OpenNMT-py is installed and check our environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch: 2.2.2+cu121\n",
            "CUDA available: True\n",
            "GPU: Tesla V100-SXM2-16GB\n",
            "GPU Memory: 15.8 GB\n",
            "\n",
            "OpenNMT-py version: 3.5.1\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x72744c905990>"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import yaml\n",
        "import torch\n",
        "\n",
        "# Check PyTorch and CUDA\n",
        "print('PyTorch:', torch.__version__)\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU:', torch.cuda.get_device_name(0))\n",
        "    print('GPU Memory:', round(torch.cuda.get_device_properties(0).total_memory / 1024**3, 1), 'GB')\n",
        "\n",
        "# Check OpenNMT-py installation\n",
        "try:\n",
        "    import onmt\n",
        "    print(f'\\nOpenNMT-py version: {onmt.__version__}')\n",
        "except ImportError:\n",
        "    print('\\nOpenNMT-py not installed. Install with: pip install OpenNMT-py')\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Download and Prepare the Data\n",
        "\n",
        "We'll use the same English-French translation dataset from Hugging Face.\n",
        "\n",
        "OpenNMT-py expects **parallel text files**:\n",
        "- `src-train.txt`: Source language sentences (one per line)\n",
        "- `tgt-train.txt`: Target language sentences (one per line)\n",
        "\n",
        "The lines must be aligned (line N in src corresponds to line N in tgt).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data already exists at data/en-fr\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "REPO_NAME = \"LT3/nfr_bt_nmt_english-french\"\n",
        "DATA_PATH = \"data/en-fr\"\n",
        "OPENNMT_DATA_PATH = \"data/opennmt-en-fr\"\n",
        "\n",
        "# Create OpenNMT data directory\n",
        "os.makedirs(OPENNMT_DATA_PATH, exist_ok=True)\n",
        "\n",
        "# Download the data if not already present\n",
        "from download_data import download_and_save_dataset\n",
        "\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    dataset_paths = download_and_save_dataset(REPO_NAME, DATA_PATH)\n",
        "else:\n",
        "    print(f\"Data already exists at {DATA_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training data: 2000 sentence pairs\n",
            "Validation data: 500 sentence pairs\n",
            "Test data: 100 sentence pairs\n"
          ]
        }
      ],
      "source": [
        "# Prepare data in OpenNMT format\n",
        "# For this tutorial, we'll use a subset of the data\n",
        "\n",
        "def prepare_opennmt_data(en_path, fr_path, out_src_path, out_tgt_path, max_samples=None):\n",
        "    \"\"\"\n",
        "    Prepare parallel data for OpenNMT-py.\n",
        "    Reads English and French files, aligns them, and writes to new files.\n",
        "    \"\"\"\n",
        "    with open(en_path, 'r', encoding='utf-8') as f:\n",
        "        en_lines = [line.strip() for line in f if line.strip()]\n",
        "    with open(fr_path, 'r', encoding='utf-8') as f:\n",
        "        fr_lines = [line.strip() for line in f if line.strip()]\n",
        "    \n",
        "    # Ensure same length\n",
        "    min_len = min(len(en_lines), len(fr_lines))\n",
        "    en_lines = en_lines[:min_len]\n",
        "    fr_lines = fr_lines[:min_len]\n",
        "    \n",
        "    if max_samples:\n",
        "        en_lines = en_lines[:max_samples]\n",
        "        fr_lines = fr_lines[:max_samples]\n",
        "    \n",
        "    # Write source (English)\n",
        "    with open(out_src_path, 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(en_lines) + '\\n')\n",
        "    \n",
        "    # Write target (French)\n",
        "    with open(out_tgt_path, 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(fr_lines) + '\\n')\n",
        "    \n",
        "    return len(en_lines)\n",
        "\n",
        "# Prepare training data (2000 examples for tutorial)\n",
        "n_train = prepare_opennmt_data(\n",
        "    f\"{DATA_PATH}/en_train.txt\",\n",
        "    f\"{DATA_PATH}/fr_train.txt\",\n",
        "    f\"{OPENNMT_DATA_PATH}/src-train.txt\",\n",
        "    f\"{OPENNMT_DATA_PATH}/tgt-train.txt\",\n",
        "    max_samples=2000\n",
        ")\n",
        "print(f\"Training data: {n_train} sentence pairs\")\n",
        "\n",
        "# Prepare validation data (500 examples)\n",
        "n_val = prepare_opennmt_data(\n",
        "    f\"{DATA_PATH}/en_validation.txt\",\n",
        "    f\"{DATA_PATH}/fr_validation.txt\",\n",
        "    f\"{OPENNMT_DATA_PATH}/src-val.txt\",\n",
        "    f\"{OPENNMT_DATA_PATH}/tgt-val.txt\",\n",
        "    max_samples=500\n",
        ")\n",
        "print(f\"Validation data: {n_val} sentence pairs\")\n",
        "\n",
        "# Prepare test data (100 examples for evaluation)\n",
        "n_test = prepare_opennmt_data(\n",
        "    f\"{DATA_PATH}/en_test.txt\",\n",
        "    f\"{DATA_PATH}/fr_test.txt\",\n",
        "    f\"{OPENNMT_DATA_PATH}/src-test.txt\",\n",
        "    f\"{OPENNMT_DATA_PATH}/tgt-test.txt\",\n",
        "    max_samples=100\n",
        ")\n",
        "print(f\"Test data: {n_test} sentence pairs\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "SAMPLE TRAINING DATA\n",
            "============================================================\n",
            "\n",
            "--- Example 1 ---\n",
            "SRC (EN): Article 199b is replaced by the following:\n",
            "TGT (FR): l'article 199 ter est remplacé par le texte suivant:\n",
            "\n",
            "--- Example 2 ---\n",
            "SRC (EN): at consular offices:\n",
            "TGT (FR): dans les bureaux consulaires:\n",
            "\n",
            "--- Example 3 ---\n",
            "SRC (EN): The Portuguese authorities have explained that this public interest mission was entrusted to the private sector in accordance with Decree-Law No 197/99 of 8 June 1999 [7], which transposed into national law European Parliament and Council Directive 97/52/EC of 13 October 1997 amending Directives 92/50/EEC, 93/36/EEC and 93/37/EEC concerning the coordination of procedures for the award of public service contracts, public supply contracts and public works contracts respectively [8].\n",
            "TGT (FR): Les autorités portugaises ont précisé que cette mission d’intérêt public avait été attribuée au secteur privé, dans le respect des prescriptions établies par le décret loi no 197/1999 du 8 juin 1999 [7], qui est l’instrument national de transposition de la directive 97/52/CE du Parlement européen et du Conseil du 13 octobre 1997 modifiant les directives 92/50/CEE, 93/36/CEE et 93/37/CEE portant coordination des procédures de passation des marchés publics de services, des marchés publics de fournitures et des marchés publics de travaux respectivement [8].\n"
          ]
        }
      ],
      "source": [
        "# Preview the data\n",
        "print(\"=\" * 60)\n",
        "print(\"SAMPLE TRAINING DATA\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "with open(f\"{OPENNMT_DATA_PATH}/src-train.txt\", 'r') as f:\n",
        "    src_lines = f.readlines()[:3]\n",
        "with open(f\"{OPENNMT_DATA_PATH}/tgt-train.txt\", 'r') as f:\n",
        "    tgt_lines = f.readlines()[:3]\n",
        "\n",
        "for i, (src, tgt) in enumerate(zip(src_lines, tgt_lines)):\n",
        "    print(f\"\\n--- Example {i+1} ---\")\n",
        "    print(f\"SRC (EN): {src.strip()}\")\n",
        "    print(f\"TGT (FR): {tgt.strip()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.5) Tokenize the Data\n",
        "\n",
        "Before building the vocabulary, we need to **tokenize** the data using a subword tokenizer.\n",
        "\n",
        "### Why tokenize?\n",
        "\n",
        "1. **Consistent vocabulary**: Subword tokenization ensures each token is a single unit (no multi-word expressions)\n",
        "2. **Handles rare words**: Rare words are split into subword units that the model can learn\n",
        "3. **Language-agnostic**: Works well across different languages\n",
        "\n",
        "We use `xlm-roberta-base` tokenizer which:\n",
        "- Uses SentencePiece under the hood\n",
        "- Is multilingual and handles both English and French well\n",
        "- Produces space-separated subword tokens\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading tokenizer (xlm-roberta-base)...\n",
            "\n",
            "Tokenizing training data...\n",
            "Tokenizing data/opennmt-en-fr/src-train.txt to data/opennmt-en-fr/src-train.tok...\n",
            "Tokenization complete.\n",
            "Tokenizing data/opennmt-en-fr/tgt-train.txt to data/opennmt-en-fr/tgt-train.tok...\n",
            "Tokenization complete.\n",
            "\n",
            "Tokenizing validation data...\n",
            "Tokenizing data/opennmt-en-fr/src-val.txt to data/opennmt-en-fr/src-val.tok...\n",
            "Tokenization complete.\n",
            "Tokenizing data/opennmt-en-fr/tgt-val.txt to data/opennmt-en-fr/tgt-val.tok...\n",
            "Tokenization complete.\n",
            "\n",
            "Tokenizing test data...\n",
            "Tokenizing data/opennmt-en-fr/src-test.txt to data/opennmt-en-fr/src-test.tok...\n",
            "Tokenization complete.\n",
            "\n",
            "============================================================\n",
            "Tokenization complete!\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Tokenize all data files using the TokenDetokenizer\n",
        "from tok_detok import TokenDetokenizer\n",
        "\n",
        "# Initialize the tokenizer (xlm-roberta-base is multilingual)\n",
        "print(\"Loading tokenizer (xlm-roberta-base)...\")\n",
        "tok_detok = TokenDetokenizer(model_name=\"xlm-roberta-base\", keep_unk=True)\n",
        "\n",
        "# Tokenize training data\n",
        "print(\"\\nTokenizing training data...\")\n",
        "tok_detok.tokenize(f\"{OPENNMT_DATA_PATH}/src-train.txt\", f\"{OPENNMT_DATA_PATH}/src-train.tok\")\n",
        "tok_detok.tokenize(f\"{OPENNMT_DATA_PATH}/tgt-train.txt\", f\"{OPENNMT_DATA_PATH}/tgt-train.tok\")\n",
        "\n",
        "# Tokenize validation data\n",
        "print(\"\\nTokenizing validation data...\")\n",
        "tok_detok.tokenize(f\"{OPENNMT_DATA_PATH}/src-val.txt\", f\"{OPENNMT_DATA_PATH}/src-val.tok\")\n",
        "tok_detok.tokenize(f\"{OPENNMT_DATA_PATH}/tgt-val.txt\", f\"{OPENNMT_DATA_PATH}/tgt-val.tok\")\n",
        "\n",
        "# Tokenize test source (we'll tokenize, translate, then detokenize)\n",
        "print(\"\\nTokenizing test data...\")\n",
        "tok_detok.tokenize(f\"{OPENNMT_DATA_PATH}/src-test.txt\", f\"{OPENNMT_DATA_PATH}/src-test.tok\")\n",
        "# Keep original target for BLEU evaluation (we compare detokenized output with original reference)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Tokenization complete!\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "SAMPLE TOKENIZED TRAINING DATA\n",
            "============================================================\n",
            "\n",
            "--- Example 1 ---\n",
            "SRC (EN tokenized): ▁Article ▁199 b ▁is ▁replace d ▁by ▁the ▁following :...\n",
            "TGT (FR tokenized): ▁l ' article ▁199 ▁ter ▁est ▁rem plac é ▁par ▁le ▁texte ▁suivant :...\n",
            "\n",
            "--- Example 2 ---\n",
            "SRC (EN tokenized): ▁at ▁consul ar ▁office s :...\n",
            "TGT (FR tokenized): ▁dans ▁les ▁bureau x ▁consul aires :...\n",
            "\n",
            "--- Example 3 ---\n",
            "SRC (EN tokenized): ▁The ▁Portu gues e ▁authorities ▁have ▁explained ▁that ▁this ▁public ▁interest ▁mission ▁was ▁en tru...\n",
            "TGT (FR tokenized): ▁Les ▁autorités ▁portu ga ises ▁ont ▁précis é ▁que ▁cette ▁mission ▁d ’ intérêt ▁public ▁avait ▁été ...\n"
          ]
        }
      ],
      "source": [
        "# Preview the tokenized data\n",
        "print(\"=\" * 60)\n",
        "print(\"SAMPLE TOKENIZED TRAINING DATA\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "with open(f\"{OPENNMT_DATA_PATH}/src-train.tok\", 'r') as f:\n",
        "    src_tok_lines = f.readlines()[:3]\n",
        "with open(f\"{OPENNMT_DATA_PATH}/tgt-train.tok\", 'r') as f:\n",
        "    tgt_tok_lines = f.readlines()[:3]\n",
        "\n",
        "for i, (src, tgt) in enumerate(zip(src_tok_lines, tgt_tok_lines)):\n",
        "    print(f\"\\n--- Example {i+1} ---\")\n",
        "    print(f\"SRC (EN tokenized): {src.strip()[:100]}...\")\n",
        "    print(f\"TGT (FR tokenized): {tgt.strip()[:100]}...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Create Configuration Files\n",
        "\n",
        "OpenNMT-py uses YAML configuration files for all settings. We'll create:\n",
        "1. **Vocabulary config**: For building source and target vocabularies\n",
        "2. **Training config**: For model architecture and training hyperparameters\n",
        "\n",
        "### Key Configuration Options\n",
        "\n",
        "- **Vocabulary**: Size, minimum frequency\n",
        "- **Model**: Transformer layers, hidden size, attention heads\n",
        "- **Training**: Batch size, learning rate, optimizer\n",
        "- **Hardware**: GPU settings, precision\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output directory: ../models/opennmt-en-fr\n",
            "Config directory: data/opennmt-en-fr/config\n"
          ]
        }
      ],
      "source": [
        "# Output directory for models and configs\n",
        "OUTPUT_DIR = \"../models/opennmt-en-fr\"\n",
        "CONFIG_DIR = f\"{OPENNMT_DATA_PATH}/config\"\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "os.makedirs(CONFIG_DIR, exist_ok=True)\n",
        "\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")\n",
        "print(f\"Config directory: {CONFIG_DIR}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary config saved to: data/opennmt-en-fr/config/vocab_config.yaml\n",
            "\n",
            "--- Vocabulary Configuration ---\n",
            "\n",
            "# Where the samples will be written\n",
            "save_data: ../models/opennmt-en-fr\n",
            "\n",
            "# Where the vocab(s) will be written\n",
            "src_vocab: ../models/opennmt-en-fr/vocab.src\n",
            "tgt_vocab: ../models/opennmt-en-fr/vocab.tgt\n",
            "\n",
            "# Prevent overwriting existing files in the folder\n",
            "overwrite: true\n",
            "\n",
            "# Corpus opts: using TOKENIZED files\n",
            "data:\n",
            "    corpus_1:\n",
            "        path_src: data/opennmt-en-fr/src-train.tok\n",
            "        path_tgt: data/opennmt-en-fr/tgt-train.tok\n",
            "    valid:\n",
            "        path_src: data/opennmt-en-fr/src-val.tok\n",
            "        path_tgt: data/opennmt-en-fr/tgt-val.tok\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Create vocabulary configuration (matching working script format)\n",
        "# Now using TOKENIZED data files (.tok)\n",
        "vocab_config_content = f\"\"\"\n",
        "# Where the samples will be written\n",
        "save_data: {OUTPUT_DIR}\n",
        "\n",
        "# Where the vocab(s) will be written\n",
        "src_vocab: {OUTPUT_DIR}/vocab.src\n",
        "tgt_vocab: {OUTPUT_DIR}/vocab.tgt\n",
        "\n",
        "# Prevent overwriting existing files in the folder\n",
        "overwrite: true\n",
        "\n",
        "# Corpus opts: using TOKENIZED files\n",
        "data:\n",
        "    corpus_1:\n",
        "        path_src: {OPENNMT_DATA_PATH}/src-train.tok\n",
        "        path_tgt: {OPENNMT_DATA_PATH}/tgt-train.tok\n",
        "    valid:\n",
        "        path_src: {OPENNMT_DATA_PATH}/src-val.tok\n",
        "        path_tgt: {OPENNMT_DATA_PATH}/tgt-val.tok\n",
        "\"\"\"\n",
        "\n",
        "# Write vocab config to file\n",
        "vocab_config_path = f'{CONFIG_DIR}/vocab_config.yaml'\n",
        "with open(vocab_config_path, 'w') as f:\n",
        "    f.write(vocab_config_content)\n",
        "\n",
        "print(f\"Vocabulary config saved to: {vocab_config_path}\")\n",
        "print(\"\\n--- Vocabulary Configuration ---\")\n",
        "print(vocab_config_content)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Build Vocabulary\n",
        "\n",
        "OpenNMT-py needs to build vocabularies before training. This creates a mapping from tokens to IDs for both source and target languages.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Removed old vocab: ../models/opennmt-en-fr/vocab.src\n",
            "Removed old vocab: ../models/opennmt-en-fr/vocab.tgt\n",
            "\n",
            "Building vocabulary...\n",
            "============================================================\n",
            "\n",
            "Warnings/Errors:\n",
            "Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
            "[2026-01-21 10:39:10,839 INFO] Counter vocab from -1 samples.\n",
            "[2026-01-21 10:39:10,839 INFO] n_sample=-1: Build vocab on full datasets.\n",
            "[2026-01-21 10:39:11,002 INFO] Counters src: 7338\n",
            "[2026-01-21 10:39:11,002 INFO] Counters tgt: 7286\n",
            "\n",
            "\n",
            "✓ Source vocab created: ../models/opennmt-en-fr/vocab.src\n",
            "✓ Target vocab created: ../models/opennmt-en-fr/vocab.tgt\n",
            "\n",
            "============================================================\n",
            "RAW VOCAB FILE CONTENT (first 10 lines of src vocab):\n",
            "============================================================\n",
            "Line 0: '▁the\\t3546'\n",
            "Line 1: '▁of\\t2299'\n",
            "Line 2: ',\\t1927'\n",
            "Line 3: 's\\t1704'\n",
            "Line 4: '.\\t1297'\n",
            "Line 5: '▁to\\t1264'\n",
            "Line 6: '▁and\\t1263'\n",
            "Line 7: '▁in\\t1146'\n",
            "Line 8: '▁for\\t584'\n",
            "Line 9: '▁a\\t575'\n"
          ]
        }
      ],
      "source": [
        "# Build vocabulary using onmt_build_vocab\n",
        "# -n_sample -1 means use ALL training data (like the working script)\n",
        "\n",
        "# First, remove old vocab files to start fresh\n",
        "for vocab_file in [f'{OUTPUT_DIR}/vocab.src', f'{OUTPUT_DIR}/vocab.tgt']:\n",
        "    if os.path.exists(vocab_file):\n",
        "        os.remove(vocab_file)\n",
        "        print(f\"Removed old vocab: {vocab_file}\")\n",
        "\n",
        "print(\"\\nBuilding vocabulary...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "result = subprocess.run(\n",
        "    [\n",
        "        'onmt_build_vocab',\n",
        "        '-config', vocab_config_path,\n",
        "        '-n_sample', '-1',  # Use ALL training data\n",
        "        '-num_threads', '2',\n",
        "    ],\n",
        "    capture_output=True,\n",
        "    text=True\n",
        ")\n",
        "\n",
        "print(result.stdout)\n",
        "if result.stderr:\n",
        "    print(\"Warnings/Errors:\")\n",
        "    print(result.stderr)\n",
        "\n",
        "# Check if vocab files were created\n",
        "if os.path.exists(f'{OUTPUT_DIR}/vocab.src'):\n",
        "    print(f\"\\n✓ Source vocab created: {OUTPUT_DIR}/vocab.src\")\n",
        "if os.path.exists(f'{OUTPUT_DIR}/vocab.tgt'):\n",
        "    print(f\"✓ Target vocab created: {OUTPUT_DIR}/vocab.tgt\")\n",
        "\n",
        "# Debug: Show raw content of vocab file\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"RAW VOCAB FILE CONTENT (first 10 lines of src vocab):\")\n",
        "print(\"=\" * 60)\n",
        "with open(f'{OUTPUT_DIR}/vocab.src', 'r', encoding='utf-8') as f:\n",
        "    for i, line in enumerate(f):\n",
        "        if i >= 10:\n",
        "            break\n",
        "        # Show the line with visible representation of tabs and special chars\n",
        "        print(f\"Line {i}: {repr(line.strip())}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source vocab size: 7338 tokens\n",
            "Target vocab size: 7286 tokens\n",
            "\n",
            "============================================================\n",
            "SOURCE VOCABULARY (first 20 tokens)\n",
            "============================================================\n",
            "▁the\t3546\n",
            "▁of\t2299\n",
            ",\t1927\n",
            "s\t1704\n",
            ".\t1297\n",
            "▁to\t1264\n",
            "▁and\t1263\n",
            "▁in\t1146\n",
            "▁for\t584\n",
            "▁a\t575\n",
            "ing\t556\n",
            "▁(\t542\n",
            "ed\t501\n",
            "▁be\t491\n",
            ")\t474\n",
            "▁\t442\n",
            "tion\t431\n",
            "▁or\t424\n",
            "▁that\t398\n",
            "-\t389\n",
            "\n",
            "... Total source vocab size: 7338\n",
            "\n",
            "============================================================\n",
            "TARGET VOCABULARY (first 20 tokens)\n",
            "============================================================\n",
            "▁de\t3225\n",
            "s\t2584\n",
            ",\t2371\n",
            "▁la\t1720\n",
            "'\t1628\n",
            "▁l\t1458\n",
            "’\t1410\n",
            "▁et\t1256\n",
            ".\t1229\n",
            "▁à\t1140\n",
            "▁des\t1139\n",
            "▁les\t1112\n",
            "▁d\t1087\n",
            "▁du\t959\n",
            "▁le\t912\n",
            "e\t782\n",
            "▁\t721\n",
            "▁en\t638\n",
            "▁dans\t465\n",
            "es\t463\n",
            "\n",
            "... Total target vocab size: 7286\n"
          ]
        }
      ],
      "source": [
        "# Count vocabulary sizes (like the working script does)\n",
        "# The working script just counts lines to set vocab_size, no modification needed\n",
        "\n",
        "def count_lines(file_path):\n",
        "    \"\"\"Count the number of lines in a file.\"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        return sum(1 for _ in f)\n",
        "\n",
        "# Count vocab sizes - will be used in training config\n",
        "SRC_VOCAB_SIZE = count_lines(f'{OUTPUT_DIR}/vocab.src')\n",
        "TGT_VOCAB_SIZE = count_lines(f'{OUTPUT_DIR}/vocab.tgt')\n",
        "\n",
        "print(f\"Source vocab size: {SRC_VOCAB_SIZE} tokens\")\n",
        "print(f\"Target vocab size: {TGT_VOCAB_SIZE} tokens\")\n",
        "\n",
        "# Preview the vocabularies\n",
        "def preview_vocab(vocab_path, n_lines=20):\n",
        "    \"\"\"Preview first n lines of a vocabulary file.\"\"\"\n",
        "    with open(vocab_path, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()[:n_lines]\n",
        "    total = count_lines(vocab_path)\n",
        "    return lines, total\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"SOURCE VOCABULARY (first 20 tokens)\")\n",
        "print(\"=\" * 60)\n",
        "src_vocab_lines, src_vocab_total = preview_vocab(f'{OUTPUT_DIR}/vocab.src')\n",
        "for line in src_vocab_lines:\n",
        "    print(line.strip())\n",
        "print(f\"\\n... Total source vocab size: {src_vocab_total}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"TARGET VOCABULARY (first 20 tokens)\")\n",
        "print(\"=\" * 60)\n",
        "tgt_vocab_lines, tgt_vocab_total = preview_vocab(f'{OUTPUT_DIR}/vocab.tgt')\n",
        "for line in tgt_vocab_lines:\n",
        "    print(line.strip())\n",
        "print(f\"\\n... Total target vocab size: {tgt_vocab_total}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Create Training Configuration\n",
        "\n",
        "Now we'll create the full training configuration. This includes:\n",
        "- Model architecture (Transformer encoder-decoder)\n",
        "- Training hyperparameters\n",
        "- Optimization settings\n",
        "\n",
        "### Transformer Architecture\n",
        "\n",
        "For this tutorial, we use a small Transformer:\n",
        "- **Encoder/Decoder layers**: 4 (default is 6)\n",
        "- **Hidden size**: 256 (default is 512)\n",
        "- **Attention heads**: 4 (default is 8)\n",
        "- **Feed-forward size**: 1024 (default is 2048)\n",
        "\n",
        "This smaller model trains faster and is suitable for our small dataset.\n",
        "\n",
        "### ⚠️ Important: Small Dataset Considerations\n",
        "\n",
        "When working with **small datasets** (< 50k sentences), you may see OpenNMT-py repeatedly logging:\n",
        "```\n",
        "Weighted corpora loaded so far: * corpus_1: N\n",
        "```\n",
        "\n",
        "This happens because OpenNMT-py's default `bucket_size` (262144 tokens) is larger than your dataset, causing it to reload the data many times to fill the bucket.\n",
        "\n",
        "**Solution**: Reduce `bucket_size` to match your dataset size. For our ~2k sentence tutorial dataset, we use `bucket_size: 32768`.\n",
        "\n",
        "See: [OpenNMT Forum Discussion](https://forum.opennmt.net/t/is-it-normal-to-see-weighted-corpora-loaded-so-far-in-a-loop)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training config saved to: data/opennmt-en-fr/config/train_config.yaml\n",
            "\n",
            "--- Training Configuration Summary ---\n",
            "  Source vocab size: 7338\n",
            "  Target vocab size: 7286\n",
            "  Model: Transformer (4 enc layers, 4 dec layers)\n",
            "  Hidden size: 256\n",
            "  Attention heads: 4\n",
            "  Training steps: 2000\n",
            "  Batch size: 32 sentences\n",
            "  Learning rate: 2.0 (with noam decay)\n",
            "  GPU: Yes\n"
          ]
        }
      ],
      "source": [
        "# Create training configuration (matching working script format)\n",
        "# Use raw YAML string to ensure proper formatting\n",
        "# Now using TOKENIZED data files (.tok)\n",
        "\n",
        "num_gpus = 1 if torch.cuda.is_available() else 0\n",
        "gpu_ranks = list(range(num_gpus)) if num_gpus > 0 else []\n",
        "\n",
        "train_config_content = f\"\"\"\n",
        "# Where the samples will be written\n",
        "save_data: {OUTPUT_DIR}\n",
        "\n",
        "# Training files - using TOKENIZED data\n",
        "data:\n",
        "    corpus_1:\n",
        "        path_src: {OPENNMT_DATA_PATH}/src-train.tok\n",
        "        path_tgt: {OPENNMT_DATA_PATH}/tgt-train.tok\n",
        "        transforms: [filtertoolong]\n",
        "    valid:\n",
        "        path_src: {OPENNMT_DATA_PATH}/src-val.tok\n",
        "        path_tgt: {OPENNMT_DATA_PATH}/tgt-val.tok\n",
        "        transforms: [filtertoolong]\n",
        "\n",
        "# Vocabulary files\n",
        "src_vocab: {OUTPUT_DIR}/vocab.src\n",
        "tgt_vocab: {OUTPUT_DIR}/vocab.tgt\n",
        "\n",
        "# Vocabulary size - must match the actual vocab file sizes\n",
        "src_vocab_size: {SRC_VOCAB_SIZE}\n",
        "tgt_vocab_size: {TGT_VOCAB_SIZE}\n",
        "\n",
        "# Filter out source/target longer than n if [filtertoolong] enabled\n",
        "src_seq_length: 200\n",
        "tgt_seq_length: 200\n",
        "\n",
        "# Logging\n",
        "log_file: {OUTPUT_DIR}/train.log\n",
        "save_model: {OUTPUT_DIR}/model\n",
        "\n",
        "# Early stopping\n",
        "early_stopping: 10\n",
        "\n",
        "# Checkpointing\n",
        "save_checkpoint_steps: 500\n",
        "keep_checkpoint: 5\n",
        "\n",
        "seed: 42\n",
        "\n",
        "# Training steps\n",
        "train_steps: 2000\n",
        "valid_steps: 500\n",
        "warmup_steps: 400\n",
        "report_every: 50\n",
        "\n",
        "# Model architecture (Transformer)\n",
        "decoder_type: transformer\n",
        "encoder_type: transformer\n",
        "word_vec_size: 256\n",
        "hidden_size: 256\n",
        "layers: 4\n",
        "transformer_ff: 1024\n",
        "heads: 4\n",
        "\n",
        "# Optimization\n",
        "accum_count: 4\n",
        "optim: adam\n",
        "adam_beta1: 0.9\n",
        "adam_beta2: 0.998\n",
        "decay_method: noam\n",
        "learning_rate: 2.0\n",
        "max_grad_norm: 0.0\n",
        "\n",
        "# Batching - use sentences for simpler setup\n",
        "batch_size: 32\n",
        "valid_batch_size: 32\n",
        "batch_type: sents\n",
        "normalization: sents\n",
        "dropout: 0.1\n",
        "label_smoothing: 0.1\n",
        "\n",
        "max_generator_batches: 2\n",
        "\n",
        "# NOTE: bucket_size is important for small datasets!\n",
        "# The default (262144) causes OpenNMT-py to reload the corpus many times\n",
        "# trying to fill the bucket. For small datasets, reduce this value.\n",
        "# See: https://forum.opennmt.net/t/is-it-normal-to-see-weighted-corpora-loaded-so-far-in-a-loop\n",
        "bucket_size: 144\n",
        "\n",
        "param_init: 0.0\n",
        "param_init_glorot: true\n",
        "position_encoding: true\n",
        "\n",
        "# Hardware\n",
        "world_size: {num_gpus}\n",
        "gpu_ranks: {gpu_ranks}\n",
        "\"\"\"\n",
        "\n",
        "# Write training config to file\n",
        "train_config_path = f'{CONFIG_DIR}/train_config.yaml'\n",
        "with open(train_config_path, 'w') as f:\n",
        "    f.write(train_config_content)\n",
        "\n",
        "print(f\"Training config saved to: {train_config_path}\")\n",
        "print(\"\\n--- Training Configuration Summary ---\")\n",
        "print(f\"  Source vocab size: {SRC_VOCAB_SIZE}\")\n",
        "print(f\"  Target vocab size: {TGT_VOCAB_SIZE}\")\n",
        "print(f\"  Model: Transformer (4 enc layers, 4 dec layers)\")\n",
        "print(f\"  Hidden size: 256\")\n",
        "print(f\"  Attention heads: 4\")\n",
        "print(f\"  Training steps: 2000\")\n",
        "print(f\"  Batch size: 32 sentences\")\n",
        "print(f\"  Learning rate: 2.0 (with noam decay)\")\n",
        "print(f\"  GPU: {'Yes' if num_gpus > 0 else 'No (CPU)'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Train the Model\n",
        "\n",
        "Now we'll train the model using `onmt_train`. \n",
        "\n",
        "Training time depends on:\n",
        "- Dataset size\n",
        "- Model size\n",
        "- Hardware (GPU vs CPU)\n",
        "\n",
        "With our small model and 2000 examples, training should take ~5-15 minutes on a GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n",
            "============================================================\n",
            "This may take several minutes...\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[2026-01-21 10:39:15,686 WARNING] Corpus corpus_1's weight should be given. We default it to 1 for you.\n",
            "[2026-01-21 10:39:15,686 INFO] Parsed 2 corpora from -data.\n",
            "[2026-01-21 10:39:15,686 INFO] Get special vocabs from Transforms: {'src': [], 'tgt': []}.\n",
            "[2026-01-21 10:39:15,732 INFO] The first 10 tokens of the vocabs are:['<unk>', '<blank>', '<s>', '</s>', '▁the', '▁of', ',', 's', '.', '▁to']\n",
            "[2026-01-21 10:39:15,732 INFO] The decoder start token is: <s>\n",
            "[2026-01-21 10:39:15,733 INFO] Building model...\n",
            "[2026-01-21 10:39:16,391 INFO] Switching model to float32 for amp/apex_amp\n",
            "[2026-01-21 10:39:16,391 INFO] Non quantized layer compute is fp32\n",
            "[2026-01-21 10:39:16,533 INFO] NMTModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(7344, 256, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding()\n",
            "      )\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (transformer): ModuleList(\n",
            "      (0-3): 4 x TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (linear_values): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (linear_query): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=256, out_features=256, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=256, out_features=1024, bias=False)\n",
            "          (w_2): Linear(in_features=1024, out_features=256, bias=False)\n",
            "          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(7288, 256, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding()\n",
            "      )\n",
            "      (dropout): Dropout(p=0.1, inplace=False)\n",
            "    )\n",
            "    (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
            "    (transformer_layers): ModuleList(\n",
            "      (0-3): 4 x TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (linear_values): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (linear_query): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=256, out_features=256, bias=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=256, out_features=1024, bias=False)\n",
            "          (w_2): Linear(in_features=1024, out_features=256, bias=False)\n",
            "          (layer_norm): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (linear_values): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (linear_query): Linear(in_features=256, out_features=256, bias=False)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=256, out_features=256, bias=False)\n",
            "        )\n",
            "        (layer_norm_2): LayerNorm((256,), eps=1e-06, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (generator): Linear(in_features=256, out_features=7288, bias=True)\n",
            ")\n",
            "[2026-01-21 10:39:16,537 INFO] encoder: 5030400\n",
            "[2026-01-21 10:39:16,537 INFO] decoder: 7939704\n",
            "[2026-01-21 10:39:16,537 INFO] * number of parameters: 12970104\n",
            "[2026-01-21 10:39:16,538 INFO] Trainable parameters = {'torch.float32': 12970104, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
            "[2026-01-21 10:39:16,538 INFO] Non trainable parameters = {'torch.float32': 0, 'torch.float16': 0, 'torch.uint8': 0, 'torch.int8': 0}\n",
            "[2026-01-21 10:39:16,538 INFO]  * src vocab size = 7344\n",
            "[2026-01-21 10:39:16,538 INFO]  * tgt vocab size = 7288\n",
            "[2026-01-21 10:39:17,106 INFO] Starting training on GPU: [0]\n",
            "[2026-01-21 10:39:17,106 INFO] Start training loop and validate every 500 steps...\n",
            "[2026-01-21 10:39:17,106 INFO] Scoring with: ['filtertoolong']\n",
            "[2026-01-21 10:39:20,999 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 1\n",
            "[2026-01-21 10:39:24,857 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 1\n",
            "[2026-01-21 10:39:24,922 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 2\n",
            "[2026-01-21 10:39:24,943 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 2\n",
            "[2026-01-21 10:39:24,996 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 3\n",
            "[2026-01-21 10:39:25,018 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 3\n",
            "[2026-01-21 10:39:25,074 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 4\n",
            "[2026-01-21 10:39:25,098 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 4\n",
            "[2026-01-21 10:39:25,156 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 5\n",
            "[2026-01-21 10:39:25,181 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 5\n",
            "[2026-01-21 10:39:25,235 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 6\n",
            "[2026-01-21 10:39:25,262 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 6\n",
            "[2026-01-21 10:39:25,308 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 7\n",
            "[2026-01-21 10:39:25,337 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 7\n",
            "[2026-01-21 10:39:25,387 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 8\n",
            "[2026-01-21 10:39:25,418 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 8\n",
            "[2026-01-21 10:39:25,465 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 9\n",
            "[2026-01-21 10:39:25,498 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 9\n",
            "[2026-01-21 10:39:25,539 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 10\n",
            "[2026-01-21 10:39:25,573 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 10\n",
            "[2026-01-21 10:39:25,617 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 11\n",
            "[2026-01-21 10:39:25,654 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 11\n",
            "[2026-01-21 10:39:25,696 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 12\n",
            "[2026-01-21 10:39:25,734 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 12\n",
            "[2026-01-21 10:39:25,777 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 13\n",
            "[2026-01-21 10:39:25,815 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 13\n",
            "[2026-01-21 10:39:25,850 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 14\n",
            "[2026-01-21 10:39:25,890 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 14\n",
            "[2026-01-21 10:39:26,111 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 15\n",
            "[2026-01-21 10:39:26,145 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 15\n",
            "[2026-01-21 10:39:26,189 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 16\n",
            "[2026-01-21 10:39:26,227 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 16\n",
            "[2026-01-21 10:39:26,263 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 17\n",
            "[2026-01-21 10:39:26,302 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 17\n",
            "[2026-01-21 10:39:26,342 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 18\n",
            "[2026-01-21 10:39:26,382 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 18\n",
            "[2026-01-21 10:39:26,423 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 19\n",
            "[2026-01-21 10:39:26,463 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 19\n",
            "[2026-01-21 10:39:26,496 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 20\n",
            "[2026-01-21 10:39:26,537 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 20\n",
            "[2026-01-21 10:39:26,576 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 21\n",
            "[2026-01-21 10:39:26,618 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 21\n",
            "[2026-01-21 10:39:26,655 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 22\n",
            "[2026-01-21 10:39:26,699 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 22\n",
            "[2026-01-21 10:39:26,734 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 23\n",
            "[2026-01-21 10:39:26,780 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 23\n",
            "[2026-01-21 10:39:26,807 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 24\n",
            "[2026-01-21 10:39:26,854 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 24\n",
            "[2026-01-21 10:39:26,886 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 25\n",
            "[2026-01-21 10:39:26,935 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 25\n",
            "[2026-01-21 10:39:26,965 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 26\n",
            "[2026-01-21 10:39:27,016 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 26\n",
            "[2026-01-21 10:39:27,041 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 27\n",
            "[2026-01-21 10:39:27,090 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 27\n",
            "[2026-01-21 10:39:27,120 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 28\n",
            "[2026-01-21 10:39:27,171 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 28\n",
            "[2026-01-21 10:39:27,199 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 29\n",
            "[2026-01-21 10:39:27,252 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 29\n",
            "[2026-01-21 10:39:27,278 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 30\n",
            "[2026-01-21 10:39:27,332 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 30\n",
            "[2026-01-21 10:39:27,653 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 31\n",
            "[2026-01-21 10:39:27,693 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 31\n",
            "[2026-01-21 10:39:27,732 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 32\n",
            "[2026-01-21 10:39:27,773 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 32\n",
            "[2026-01-21 10:39:27,811 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 33\n",
            "[2026-01-21 10:39:27,854 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 33\n",
            "[2026-01-21 10:39:30,406 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 34\n",
            "[2026-01-21 10:39:30,481 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 35\n",
            "[2026-01-21 10:39:30,495 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 34\n",
            "[2026-01-21 10:39:30,579 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 35\n",
            "[2026-01-21 10:39:31,048 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 36\n",
            "[2026-01-21 10:39:31,089 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 36\n",
            "[2026-01-21 10:39:31,131 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 37\n",
            "[2026-01-21 10:39:31,166 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 37\n",
            "[2026-01-21 10:39:31,200 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 38\n",
            "[2026-01-21 10:39:31,242 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 38\n",
            "[2026-01-21 10:39:31,279 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 39\n",
            "[2026-01-21 10:39:31,311 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 39\n",
            "[2026-01-21 10:39:31,355 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 40\n",
            "[2026-01-21 10:39:31,389 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 40\n",
            "[2026-01-21 10:39:31,424 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 41\n",
            "[2026-01-21 10:39:31,467 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 41\n",
            "[2026-01-21 10:39:31,500 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 42\n",
            "[2026-01-21 10:39:31,541 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 42\n",
            "[2026-01-21 10:39:31,577 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 43\n",
            "[2026-01-21 10:39:31,610 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 43\n",
            "[2026-01-21 10:39:31,653 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 44\n",
            "[2026-01-21 10:39:31,685 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 44\n",
            "[2026-01-21 10:39:31,723 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 45\n",
            "[2026-01-21 10:39:31,760 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 45\n",
            "[2026-01-21 10:39:31,800 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 46\n",
            "[2026-01-21 10:39:31,842 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 46\n",
            "[2026-01-21 10:39:31,878 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 47\n",
            "[2026-01-21 10:39:31,915 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 47\n",
            "[2026-01-21 10:39:31,968 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 48\n",
            "[2026-01-21 10:39:31,992 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 48\n",
            "[2026-01-21 10:39:32,044 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 49\n",
            "[2026-01-21 10:39:32,071 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 49\n",
            "[2026-01-21 10:39:32,128 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 50\n",
            "[2026-01-21 10:39:32,149 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 50\n",
            "[2026-01-21 10:39:32,211 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 51\n",
            "[2026-01-21 10:39:32,224 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 51\n",
            "[2026-01-21 10:39:32,288 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 52\n",
            "[2026-01-21 10:39:32,304 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 52\n",
            "[2026-01-21 10:39:32,372 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 53\n",
            "[2026-01-21 10:39:32,384 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 53\n",
            "[2026-01-21 10:39:32,455 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 54\n",
            "[2026-01-21 10:39:32,463 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 54\n",
            "[2026-01-21 10:39:32,537 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 55\n",
            "[2026-01-21 10:39:32,537 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 55\n",
            "[2026-01-21 10:39:32,620 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 56\n",
            "[2026-01-21 10:39:33,350 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 56\n",
            "[2026-01-21 10:39:33,368 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 57\n",
            "[2026-01-21 10:39:33,430 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 57\n",
            "[2026-01-21 10:39:33,451 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 58\n",
            "[2026-01-21 10:39:33,513 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 58\n",
            "[2026-01-21 10:39:33,538 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 59\n",
            "[2026-01-21 10:39:33,591 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 59\n",
            "[2026-01-21 10:39:33,613 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 60\n",
            "[2026-01-21 10:39:33,672 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 60\n",
            "[2026-01-21 10:39:33,695 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 61\n",
            "[2026-01-21 10:39:33,753 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 61\n",
            "[2026-01-21 10:39:33,779 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 62\n",
            "[2026-01-21 10:39:33,836 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 62\n",
            "[2026-01-21 10:39:33,861 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 63\n",
            "[2026-01-21 10:39:33,913 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 63\n",
            "[2026-01-21 10:39:33,937 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 64\n",
            "[2026-01-21 10:39:33,994 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 64\n",
            "[2026-01-21 10:39:34,020 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 65\n",
            "[2026-01-21 10:39:34,076 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 65\n",
            "[2026-01-21 10:39:34,102 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 66\n",
            "[2026-01-21 10:39:34,158 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 66\n",
            "[2026-01-21 10:39:37,654 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 67\n",
            "[2026-01-21 10:39:37,679 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 67\n",
            "[2026-01-21 10:39:37,728 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 68\n",
            "[2026-01-21 10:39:37,752 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 68\n",
            "[2026-01-21 10:39:37,806 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 69\n",
            "[2026-01-21 10:39:37,829 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 69\n",
            "[2026-01-21 10:39:37,875 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 70\n",
            "[2026-01-21 10:39:37,897 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 70\n",
            "[2026-01-21 10:39:37,952 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 71\n",
            "[2026-01-21 10:39:37,976 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 71\n",
            "[2026-01-21 10:39:38,027 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 72\n",
            "[2026-01-21 10:39:38,053 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 72\n",
            "[2026-01-21 10:39:38,096 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 73\n",
            "[2026-01-21 10:39:38,128 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 73\n",
            "[2026-01-21 10:39:38,171 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 74\n",
            "[2026-01-21 10:39:38,196 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 74\n",
            "[2026-01-21 10:39:38,246 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 75\n",
            "[2026-01-21 10:39:38,321 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 76\n",
            "[2026-01-21 10:39:38,835 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 75\n",
            "[2026-01-21 10:39:38,908 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 76\n",
            "[2026-01-21 10:39:38,975 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 77\n",
            "[2026-01-21 10:39:38,977 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 77\n",
            "[2026-01-21 10:39:39,050 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 78\n",
            "[2026-01-21 10:39:39,053 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 78\n",
            "[2026-01-21 10:39:39,124 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 79\n",
            "[2026-01-21 10:39:39,127 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 79\n",
            "[2026-01-21 10:39:39,191 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 80\n",
            "[2026-01-21 10:39:39,196 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 80\n",
            "[2026-01-21 10:39:39,265 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 81\n",
            "[2026-01-21 10:39:39,270 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 81\n",
            "[2026-01-21 10:39:39,339 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 82\n",
            "[2026-01-21 10:39:39,345 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 82\n",
            "[2026-01-21 10:39:39,412 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 83\n",
            "[2026-01-21 10:39:39,413 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 83\n",
            "[2026-01-21 10:39:39,479 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 84\n",
            "[2026-01-21 10:39:39,488 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 84\n",
            "[2026-01-21 10:39:39,552 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 85\n",
            "[2026-01-21 10:39:39,564 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 85\n",
            "[2026-01-21 10:39:39,625 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 86\n",
            "[2026-01-21 10:39:39,639 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 86\n",
            "[2026-01-21 10:39:39,692 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 87\n",
            "[2026-01-21 10:39:39,708 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 87\n",
            "[2026-01-21 10:39:39,766 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 88\n",
            "[2026-01-21 10:39:39,782 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 88\n",
            "[2026-01-21 10:39:39,840 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 89\n",
            "[2026-01-21 10:39:39,857 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 89\n",
            "[2026-01-21 10:39:39,914 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 90\n",
            "[2026-01-21 10:39:39,925 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 90\n",
            "[2026-01-21 10:39:39,982 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 91\n",
            "[2026-01-21 10:39:40,001 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 91\n",
            "[2026-01-21 10:39:40,055 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 92\n",
            "[2026-01-21 10:39:40,077 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 92\n",
            "[2026-01-21 10:39:40,129 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 93\n",
            "[2026-01-21 10:39:40,152 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 93\n",
            "[2026-01-21 10:39:40,196 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 94\n",
            "[2026-01-21 10:39:40,220 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 94\n",
            "[2026-01-21 10:39:40,269 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 95\n",
            "[2026-01-21 10:39:40,295 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 95\n",
            "[2026-01-21 10:39:40,343 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 96\n",
            "[2026-01-21 10:39:40,370 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 96\n",
            "[2026-01-21 10:39:40,416 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 97\n",
            "[2026-01-21 10:39:40,439 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 97\n",
            "[2026-01-21 10:39:40,484 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 98\n",
            "[2026-01-21 10:39:40,514 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 98\n",
            "[2026-01-21 10:39:40,590 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 99\n",
            "[2026-01-21 10:39:41,227 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 99\n",
            "[2026-01-21 10:39:44,242 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 100\n",
            "[2026-01-21 10:39:44,258 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 100\n",
            "[2026-01-21 10:39:44,310 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 101\n",
            "[2026-01-21 10:39:44,331 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 101\n",
            "[2026-01-21 10:39:44,388 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 102\n",
            "[2026-01-21 10:39:44,408 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 102\n",
            "[2026-01-21 10:39:44,465 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 103\n",
            "[2026-01-21 10:39:44,476 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 103\n",
            "[2026-01-21 10:39:44,533 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 104\n",
            "[2026-01-21 10:39:44,552 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 104\n",
            "[2026-01-21 10:39:44,609 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 105\n",
            "[2026-01-21 10:39:44,627 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 105\n",
            "[2026-01-21 10:39:44,685 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 106\n",
            "[2026-01-21 10:39:44,702 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 106\n",
            "[2026-01-21 10:39:44,753 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 107\n",
            "[2026-01-21 10:39:44,770 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 107\n",
            "[2026-01-21 10:39:44,829 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 108\n",
            "[2026-01-21 10:39:44,846 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 108\n",
            "[2026-01-21 10:39:44,904 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 109\n",
            "[2026-01-21 10:39:44,921 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 109\n",
            "[2026-01-21 10:39:44,964 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 110\n",
            "[2026-01-21 10:39:44,990 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 110\n",
            "[2026-01-21 10:39:45,020 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 111\n",
            "[2026-01-21 10:39:45,066 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 111\n",
            "[2026-01-21 10:39:45,082 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 112\n",
            "[2026-01-21 10:39:45,141 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 112\n",
            "[2026-01-21 10:39:45,156 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 113\n",
            "[2026-01-21 10:39:45,217 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 113\n",
            "[2026-01-21 10:39:45,225 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 114\n",
            "[2026-01-21 10:39:45,286 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 114\n",
            "[2026-01-21 10:39:45,300 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 115\n",
            "[2026-01-21 10:39:45,363 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 115\n",
            "[2026-01-21 10:39:45,374 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 116\n",
            "[2026-01-21 10:39:45,438 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 116\n",
            "[2026-01-21 10:39:45,441 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 117\n",
            "[2026-01-21 10:39:45,507 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 117\n",
            "[2026-01-21 10:39:45,584 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 118\n",
            "[2026-01-21 10:39:46,196 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 118\n",
            "[2026-01-21 10:39:46,246 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 119\n",
            "[2026-01-21 10:39:46,270 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 119\n",
            "[2026-01-21 10:39:46,316 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 120\n",
            "[2026-01-21 10:39:46,343 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 120\n",
            "[2026-01-21 10:39:46,391 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 121\n",
            "[2026-01-21 10:39:46,410 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 121\n",
            "[2026-01-21 10:39:46,466 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 122\n",
            "[2026-01-21 10:39:46,483 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 122\n",
            "[2026-01-21 10:39:46,540 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 123\n",
            "[2026-01-21 10:39:46,556 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 123\n",
            "[2026-01-21 10:39:46,609 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 124\n",
            "[2026-01-21 10:39:46,623 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 124\n",
            "[2026-01-21 10:39:46,685 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 125\n",
            "[2026-01-21 10:39:46,695 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 125\n",
            "[2026-01-21 10:39:46,760 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 126\n",
            "[2026-01-21 10:39:46,766 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 126\n",
            "[2026-01-21 10:39:46,829 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 127\n",
            "[2026-01-21 10:39:46,839 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 127\n",
            "[2026-01-21 10:39:46,904 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 128\n",
            "[2026-01-21 10:39:46,906 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 128\n",
            "[2026-01-21 10:39:46,979 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 129\n",
            "[2026-01-21 10:39:46,981 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 129\n",
            "[2026-01-21 10:39:47,054 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 130\n",
            "[2026-01-21 10:39:47,056 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 130\n",
            "[2026-01-21 10:39:47,123 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 131\n",
            "[2026-01-21 10:39:47,124 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 131\n",
            "[2026-01-21 10:39:47,198 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 132\n",
            "[2026-01-21 10:39:47,198 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 132\n",
            "[2026-01-21 10:39:50,323 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 133\n",
            "[2026-01-21 10:39:50,392 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 134\n",
            "[2026-01-21 10:39:50,398 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 133\n",
            "[2026-01-21 10:39:50,466 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 134\n",
            "[2026-01-21 10:39:50,469 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 135\n",
            "[2026-01-21 10:39:50,543 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 135\n",
            "[2026-01-21 10:39:50,544 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 136\n",
            "[2026-01-21 10:39:50,613 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 137\n",
            "[2026-01-21 10:39:50,618 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 136\n",
            "[2026-01-21 10:39:50,688 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 138\n",
            "[2026-01-21 10:39:50,689 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 137\n",
            "[2026-01-21 10:39:50,763 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 139\n",
            "[2026-01-21 10:39:50,764 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 138\n",
            "[2026-01-21 10:39:50,838 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 140\n",
            "[2026-01-21 10:39:50,840 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 139\n",
            "[2026-01-21 10:39:50,908 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 140\n",
            "[2026-01-21 10:39:51,430 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 141\n",
            "[2026-01-21 10:39:51,512 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 142\n",
            "[2026-01-21 10:39:51,535 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 141\n",
            "[2026-01-21 10:39:51,588 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 143\n",
            "[2026-01-21 10:39:51,610 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 142\n",
            "[2026-01-21 10:39:51,659 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 144\n",
            "[2026-01-21 10:39:51,685 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 143\n",
            "[2026-01-21 10:39:51,735 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 145\n",
            "[2026-01-21 10:39:51,754 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 144\n",
            "[2026-01-21 10:39:51,798 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 146\n",
            "[2026-01-21 10:39:51,829 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 145\n",
            "[2026-01-21 10:39:51,852 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 147\n",
            "[2026-01-21 10:39:51,905 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 146\n",
            "[2026-01-21 10:39:51,913 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 148\n",
            "[2026-01-21 10:39:51,973 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 147\n",
            "[2026-01-21 10:39:51,977 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 149\n",
            "[2026-01-21 10:39:52,039 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 150\n",
            "[2026-01-21 10:39:52,048 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 148\n",
            "[2026-01-21 10:39:52,095 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 151\n",
            "[2026-01-21 10:39:52,123 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 149\n",
            "[2026-01-21 10:39:52,158 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 152\n",
            "[2026-01-21 10:39:52,198 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 150\n",
            "[2026-01-21 10:39:52,219 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 153\n",
            "[2026-01-21 10:39:52,267 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 151\n",
            "[2026-01-21 10:39:52,276 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 154\n",
            "[2026-01-21 10:39:52,341 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 155\n",
            "[2026-01-21 10:39:52,342 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 152\n",
            "[2026-01-21 10:39:52,399 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 156\n",
            "[2026-01-21 10:39:52,417 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 153\n",
            "[2026-01-21 10:39:52,461 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 157\n",
            "[2026-01-21 10:39:52,485 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 154\n",
            "[2026-01-21 10:39:52,517 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 158\n",
            "[2026-01-21 10:39:52,561 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 155\n",
            "[2026-01-21 10:39:52,579 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 159\n",
            "[2026-01-21 10:39:52,635 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 156\n",
            "[2026-01-21 10:39:52,642 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 160\n",
            "[2026-01-21 10:39:52,696 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 161\n",
            "[2026-01-21 10:39:52,710 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 157\n",
            "[2026-01-21 10:39:52,760 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 162\n",
            "[2026-01-21 10:39:52,778 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 158\n",
            "[2026-01-21 10:39:52,854 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 159\n",
            "[2026-01-21 10:39:52,929 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 160\n",
            "[2026-01-21 10:39:52,997 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 161\n",
            "[2026-01-21 10:39:53,073 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 162\n",
            "[2026-01-21 10:39:53,148 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 163\n",
            "[2026-01-21 10:39:53,223 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 164\n",
            "[2026-01-21 10:39:53,571 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 163\n",
            "[2026-01-21 10:39:53,633 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 164\n",
            "[2026-01-21 10:39:54,337 INFO] Step 50/ 2000; acc: 3.5; ppl: 2611.7; xent: 7.9; lr: 0.00080; sents:    6343; bsz: 1019/1291/32; 5476/6936 tok/s;     37 sec;\n",
            "[2026-01-21 10:39:56,580 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 165\n",
            "[2026-01-21 10:39:56,653 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 166\n",
            "[2026-01-21 10:39:56,730 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 167\n",
            "[2026-01-21 10:39:56,775 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 165\n",
            "[2026-01-21 10:39:56,798 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 168\n",
            "[2026-01-21 10:39:56,837 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 166\n",
            "[2026-01-21 10:39:56,874 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 169\n",
            "[2026-01-21 10:39:56,892 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 167\n",
            "[2026-01-21 10:39:56,950 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 170\n",
            "[2026-01-21 10:39:56,957 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 168\n",
            "[2026-01-21 10:39:57,019 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 171\n",
            "[2026-01-21 10:39:57,020 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 169\n",
            "[2026-01-21 10:39:57,082 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 170\n",
            "[2026-01-21 10:39:57,096 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 172\n",
            "[2026-01-21 10:39:57,134 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 171\n",
            "[2026-01-21 10:39:57,172 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 173\n",
            "[2026-01-21 10:39:57,201 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 172\n",
            "[2026-01-21 10:39:57,247 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 174\n",
            "[2026-01-21 10:39:57,265 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 173\n",
            "[2026-01-21 10:39:57,316 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 175\n",
            "[2026-01-21 10:39:57,317 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 174\n",
            "[2026-01-21 10:39:57,380 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 175\n",
            "[2026-01-21 10:39:57,391 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 176\n",
            "[2026-01-21 10:39:57,444 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 176\n",
            "[2026-01-21 10:39:57,467 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 177\n",
            "[2026-01-21 10:39:57,504 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 177\n",
            "[2026-01-21 10:39:57,536 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 178\n",
            "[2026-01-21 10:39:57,562 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 178\n",
            "[2026-01-21 10:39:57,611 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 179\n",
            "[2026-01-21 10:39:57,622 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 179\n",
            "[2026-01-21 10:39:57,683 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 180\n",
            "[2026-01-21 10:39:57,686 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 180\n",
            "[2026-01-21 10:39:57,742 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 181\n",
            "[2026-01-21 10:39:57,762 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 181\n",
            "[2026-01-21 10:39:57,802 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 182\n",
            "[2026-01-21 10:39:57,830 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 182\n",
            "[2026-01-21 10:39:58,474 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 183\n",
            "[2026-01-21 10:39:58,500 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 183\n",
            "[2026-01-21 10:39:58,543 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 184\n",
            "[2026-01-21 10:39:58,575 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 184\n",
            "[2026-01-21 10:39:58,618 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 185\n",
            "[2026-01-21 10:39:58,644 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 185\n",
            "[2026-01-21 10:39:58,693 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 186\n",
            "[2026-01-21 10:39:58,719 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 186\n",
            "[2026-01-21 10:39:58,768 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 187\n",
            "[2026-01-21 10:39:58,794 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 187\n",
            "[2026-01-21 10:39:58,837 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 188\n",
            "[2026-01-21 10:39:58,862 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 188\n",
            "[2026-01-21 10:39:58,911 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 189\n",
            "[2026-01-21 10:39:58,937 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 189\n",
            "[2026-01-21 10:39:58,987 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 190\n",
            "[2026-01-21 10:39:59,012 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 190\n",
            "[2026-01-21 10:39:59,055 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 191\n",
            "[2026-01-21 10:39:59,087 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 191\n",
            "[2026-01-21 10:39:59,130 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 192\n",
            "[2026-01-21 10:39:59,165 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 192\n",
            "[2026-01-21 10:39:59,205 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 193\n",
            "[2026-01-21 10:39:59,246 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 193\n",
            "[2026-01-21 10:39:59,279 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 194\n",
            "[2026-01-21 10:39:59,322 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 194\n",
            "[2026-01-21 10:39:59,348 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 195\n",
            "[2026-01-21 10:39:59,391 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 195\n",
            "[2026-01-21 10:39:59,424 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 196\n",
            "[2026-01-21 10:39:59,467 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 196\n",
            "[2026-01-21 10:39:59,498 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 197\n",
            "[2026-01-21 10:39:59,542 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 197\n",
            "[2026-01-21 10:40:02,673 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 198\n",
            "[2026-01-21 10:40:02,688 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 198\n",
            "[2026-01-21 10:40:02,746 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 199\n",
            "[2026-01-21 10:40:02,757 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 199\n",
            "[2026-01-21 10:40:02,825 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 200\n",
            "[2026-01-21 10:40:02,835 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 200\n",
            "[2026-01-21 10:40:02,894 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 201\n",
            "[2026-01-21 10:40:02,911 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 201\n",
            "[2026-01-21 10:40:02,971 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 202\n",
            "[2026-01-21 10:40:02,979 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 202\n",
            "[2026-01-21 10:40:03,047 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 203\n",
            "[2026-01-21 10:40:03,055 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 203\n",
            "[2026-01-21 10:40:03,108 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 204\n",
            "[2026-01-21 10:40:03,130 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 204\n",
            "[2026-01-21 10:40:03,171 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 205\n",
            "[2026-01-21 10:40:03,199 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 205\n",
            "[2026-01-21 10:40:03,775 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 206\n",
            "[2026-01-21 10:40:03,799 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 206\n",
            "[2026-01-21 10:40:03,833 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 207\n",
            "[2026-01-21 10:40:03,874 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 207\n",
            "[2026-01-21 10:40:03,890 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 208\n",
            "[2026-01-21 10:40:03,943 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 208\n",
            "[2026-01-21 10:40:03,954 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 209\n",
            "[2026-01-21 10:40:04,018 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 210\n",
            "[2026-01-21 10:40:04,019 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 209\n",
            "[2026-01-21 10:40:04,073 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 211\n",
            "[2026-01-21 10:40:04,094 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 210\n",
            "[2026-01-21 10:40:04,136 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 212\n",
            "[2026-01-21 10:40:04,169 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 211\n",
            "[2026-01-21 10:40:04,195 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 213\n",
            "[2026-01-21 10:40:04,237 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 212\n",
            "[2026-01-21 10:40:04,260 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 214\n",
            "[2026-01-21 10:40:04,313 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 213\n",
            "[2026-01-21 10:40:04,319 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 215\n",
            "[2026-01-21 10:40:04,377 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 216\n",
            "[2026-01-21 10:40:04,388 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 214\n",
            "[2026-01-21 10:40:04,440 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 217\n",
            "[2026-01-21 10:40:04,457 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 215\n",
            "[2026-01-21 10:40:04,498 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 218\n",
            "[2026-01-21 10:40:04,533 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 216\n",
            "[2026-01-21 10:40:04,561 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 219\n",
            "[2026-01-21 10:40:04,608 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 217\n",
            "[2026-01-21 10:40:04,622 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 220\n",
            "[2026-01-21 10:40:04,683 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 218\n",
            "[2026-01-21 10:40:04,700 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 221\n",
            "[2026-01-21 10:40:04,752 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 219\n",
            "[2026-01-21 10:40:04,770 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 222\n",
            "[2026-01-21 10:40:04,826 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 220\n",
            "[2026-01-21 10:40:04,847 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 223\n",
            "[2026-01-21 10:40:04,902 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 221\n",
            "[2026-01-21 10:40:04,923 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 224\n",
            "[2026-01-21 10:40:04,970 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 222\n",
            "[2026-01-21 10:40:04,993 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 225\n",
            "[2026-01-21 10:40:05,045 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 223\n",
            "[2026-01-21 10:40:05,070 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 226\n",
            "[2026-01-21 10:40:05,120 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 224\n",
            "[2026-01-21 10:40:05,146 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 227\n",
            "[2026-01-21 10:40:05,196 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 225\n",
            "[2026-01-21 10:40:05,224 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 228\n",
            "[2026-01-21 10:40:05,263 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 226\n",
            "[2026-01-21 10:40:05,337 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 227\n",
            "[2026-01-21 10:40:05,983 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 229\n",
            "[2026-01-21 10:40:06,058 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 230\n",
            "[2026-01-21 10:40:06,093 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 228\n",
            "[2026-01-21 10:40:06,162 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 229\n",
            "[2026-01-21 10:40:06,237 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 230\n",
            "[2026-01-21 10:40:08,885 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 231\n",
            "[2026-01-21 10:40:08,955 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 232\n",
            "[2026-01-21 10:40:08,962 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 231\n",
            "[2026-01-21 10:40:09,032 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 233\n",
            "[2026-01-21 10:40:09,034 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 232\n",
            "[2026-01-21 10:40:09,108 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 234\n",
            "[2026-01-21 10:40:09,113 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 233\n",
            "[2026-01-21 10:40:09,177 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 235\n",
            "[2026-01-21 10:40:09,193 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 234\n",
            "[2026-01-21 10:40:09,254 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 236\n",
            "[2026-01-21 10:40:09,265 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 235\n",
            "[2026-01-21 10:40:09,329 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 237\n",
            "[2026-01-21 10:40:09,344 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 236\n",
            "[2026-01-21 10:40:09,405 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 238\n",
            "[2026-01-21 10:40:09,421 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 237\n",
            "[2026-01-21 10:40:09,473 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 239\n",
            "[2026-01-21 10:40:09,492 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 238\n",
            "[2026-01-21 10:40:09,549 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 240\n",
            "[2026-01-21 10:40:09,571 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 239\n",
            "[2026-01-21 10:40:09,624 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 241\n",
            "[2026-01-21 10:40:09,648 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 240\n",
            "[2026-01-21 10:40:09,693 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 242\n",
            "[2026-01-21 10:40:09,728 INFO] Weighted corpora loaded so far:\n",
            "\t\t\t* corpus_1: 241\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/thomas/Projects/starter-kit/mt_starter_kit_venv/bin/onmt_train\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/home/thomas/Projects/starter-kit/mt_starter_kit_venv/lib/python3.12/site-packages/onmt/bin/train.py\", line 67, in main\n",
            "    train(opt)\n",
            "  File \"/home/thomas/Projects/starter-kit/mt_starter_kit_venv/lib/python3.12/site-packages/onmt/bin/train.py\", line 52, in train\n",
            "    train_process(opt, device_id=0)\n",
            "  File \"/home/thomas/Projects/starter-kit/mt_starter_kit_venv/lib/python3.12/site-packages/onmt/train_single.py\", line 238, in main\n",
            "    trainer.train(\n",
            "  File \"/home/thomas/Projects/starter-kit/mt_starter_kit_venv/lib/python3.12/site-packages/onmt/trainer.py\", line 308, in train\n",
            "    for i, (batches, normalization) in enumerate(self._accum_batches(train_iter)):\n",
            "  File \"/home/thomas/Projects/starter-kit/mt_starter_kit_venv/lib/python3.12/site-packages/onmt/trainer.py\", line 238, in _accum_batches\n",
            "    for batch, bucket_idx in iterator:\n",
            "  File \"/home/thomas/Projects/starter-kit/mt_starter_kit_venv/lib/python3.12/site-packages/onmt/inputters/dynamic_iterator.py\", line 373, in __iter__\n",
            "    for (tensor_batch, bucket_idx) in self.data_iter:\n",
            "  File \"/home/thomas/Projects/starter-kit/mt_starter_kit_venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 631, in __next__\n",
            "    data = self._next_data()\n",
            "           ^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/thomas/Projects/starter-kit/mt_starter_kit_venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1329, in _next_data\n",
            "    idx, data = self._get_data()\n",
            "                ^^^^^^^^^^^^^^^^\n",
            "  File \"/home/thomas/Projects/starter-kit/mt_starter_kit_venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1285, in _get_data\n",
            "    success, data = self._try_get_data()\n",
            "                    ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/home/thomas/Projects/starter-kit/mt_starter_kit_venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py\", line 1133, in _try_get_data\n",
            "    data = self._data_queue.get(timeout=timeout)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/queue.py\", line 180, in get\n",
            "    self.not_empty.wait(remaining)\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 359, in wait\n",
            "    gotit = waiter.acquire(True, timeout)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "Exception in thread Thread-1 (_pin_memory_loop):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1073, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 1010, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/home/thomas/Projects/starter-kit/mt_starter_kit_venv/lib/python3.12/site-packages/torch/utils/data/_utils/pin_memory.py\", line 53, in _pin_memory_loop\n",
            "    do_one_step()\n",
            "  File \"/home/thomas/Projects/starter-kit/mt_starter_kit_venv/lib/python3.12/site-packages/torch/utils/data/_utils/pin_memory.py\", line 30, in do_one_step\n",
            "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
            "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/multiprocessing/queues.py\", line 122, in get\n",
            "    return _ForkingPickler.loads(res)\n",
            " "
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Run training\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m result = \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43monmt_train\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m-config\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_config_path\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Show output in real-time\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     12\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining complete!\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/subprocess.py:550\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    548\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Popen(*popenargs, **kwargs) \u001b[38;5;28;01mas\u001b[39;00m process:\n\u001b[32m    549\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m550\u001b[39m         stdout, stderr = \u001b[43mprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcommunicate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    551\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m TimeoutExpired \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    552\u001b[39m         process.kill()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/subprocess.py:1201\u001b[39m, in \u001b[36mPopen.communicate\u001b[39m\u001b[34m(self, input, timeout)\u001b[39m\n\u001b[32m   1199\u001b[39m         stderr = \u001b[38;5;28mself\u001b[39m.stderr.read()\n\u001b[32m   1200\u001b[39m         \u001b[38;5;28mself\u001b[39m.stderr.close()\n\u001b[32m-> \u001b[39m\u001b[32m1201\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1203\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/subprocess.py:1264\u001b[39m, in \u001b[36mPopen.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1262\u001b[39m     endtime = _time() + timeout\n\u001b[32m   1263\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1264\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1265\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m   1266\u001b[39m     \u001b[38;5;66;03m# https://bugs.python.org/issue25942\u001b[39;00m\n\u001b[32m   1267\u001b[39m     \u001b[38;5;66;03m# The first keyboard interrupt waits briefly for the child to\u001b[39;00m\n\u001b[32m   1268\u001b[39m     \u001b[38;5;66;03m# exit under the common assumption that it also received the ^C\u001b[39;00m\n\u001b[32m   1269\u001b[39m     \u001b[38;5;66;03m# generated SIGINT and will exit rapidly.\u001b[39;00m\n\u001b[32m   1270\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/subprocess.py:2053\u001b[39m, in \u001b[36mPopen._wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   2051\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.returncode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2052\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Another thread waited.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2053\u001b[39m (pid, sts) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2054\u001b[39m \u001b[38;5;66;03m# Check the pid and loop as waitpid has been known to\u001b[39;00m\n\u001b[32m   2055\u001b[39m \u001b[38;5;66;03m# return 0 even without WNOHANG in odd situations.\u001b[39;00m\n\u001b[32m   2056\u001b[39m \u001b[38;5;66;03m# http://bugs.python.org/issue14396.\u001b[39;00m\n\u001b[32m   2057\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m pid == \u001b[38;5;28mself\u001b[39m.pid:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/subprocess.py:2011\u001b[39m, in \u001b[36mPopen._try_wait\u001b[39m\u001b[34m(self, wait_flags)\u001b[39m\n\u001b[32m   2009\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"All callers to this function MUST hold self._waitpid_lock.\"\"\"\u001b[39;00m\n\u001b[32m   2010\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2011\u001b[39m     (pid, sts) = \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwaitpid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwait_flags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2012\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mChildProcessError\u001b[39;00m:\n\u001b[32m   2013\u001b[39m     \u001b[38;5;66;03m# This happens if SIGCLD is set to be ignored or waiting\u001b[39;00m\n\u001b[32m   2014\u001b[39m     \u001b[38;5;66;03m# for child processes has otherwise been disabled for our\u001b[39;00m\n\u001b[32m   2015\u001b[39m     \u001b[38;5;66;03m# process.  This child is dead, we can't get the status.\u001b[39;00m\n\u001b[32m   2016\u001b[39m     pid = \u001b[38;5;28mself\u001b[39m.pid\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "print(\"Starting training...\")\n",
        "print(\"=\" * 60)\n",
        "print(\"This may take several minutes...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Run training\n",
        "result = subprocess.run(\n",
        "    ['onmt_train', '-config', train_config_path],\n",
        "    capture_output=False,  # Show output in real-time\n",
        "    text=True\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"Training complete!\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List saved checkpoints\n",
        "print(\"Saved model checkpoints:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "checkpoints = sorted([f for f in os.listdir(OUTPUT_DIR) if f.startswith('model_step_')])\n",
        "for ckpt in checkpoints:\n",
        "    path = os.path.join(OUTPUT_DIR, ckpt)\n",
        "    size_mb = os.path.getsize(path) / (1024 * 1024)\n",
        "    print(f\"  {ckpt}: {size_mb:.1f} MB\")\n",
        "\n",
        "# Get the latest checkpoint\n",
        "if checkpoints:\n",
        "    BEST_MODEL = os.path.join(OUTPUT_DIR, checkpoints[-1])\n",
        "    print(f\"\\nLatest checkpoint: {BEST_MODEL}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Translate with the Trained Model\n",
        "\n",
        "Now let's use our trained model to translate the test set. We use `onmt_translate` for inference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create translation configuration\n",
        "# Using TOKENIZED test source - output will be tokenized too\n",
        "translate_config = {\n",
        "    'model': BEST_MODEL,\n",
        "    'src': f'{OPENNMT_DATA_PATH}/src-test.tok',  # Tokenized input\n",
        "    'output': f'{OUTPUT_DIR}/predictions.tok',   # Tokenized output\n",
        "    \n",
        "    # Decoding settings\n",
        "    'beam_size': 5,\n",
        "    'max_length': 200,\n",
        "    \n",
        "    # GPU\n",
        "    'gpu': 0 if torch.cuda.is_available() else -1,\n",
        "    \n",
        "    # Batch size for inference\n",
        "    'batch_size': 32,\n",
        "    'batch_type': 'sents',\n",
        "    \n",
        "    # Verbosity\n",
        "    'verbose': True,\n",
        "}\n",
        "\n",
        "# Write translate config\n",
        "translate_config_path = f'{CONFIG_DIR}/translate_config.yaml'\n",
        "with open(translate_config_path, 'w') as f:\n",
        "    yaml.dump(translate_config, f, default_flow_style=False)\n",
        "\n",
        "print(f\"Translation config saved to: {translate_config_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run translation\n",
        "print(\"Translating test set...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "result = subprocess.run(\n",
        "    ['onmt_translate', '-config', translate_config_path],\n",
        "    capture_output=True,\n",
        "    text=True\n",
        ")\n",
        "\n",
        "print(result.stdout)\n",
        "if result.stderr:\n",
        "    # Filter out just the important info\n",
        "    for line in result.stderr.split('\\n'):\n",
        "        if 'PRED' not in line:  # Skip individual predictions in verbose mode\n",
        "            print(line)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(f\"Tokenized translations saved to: {OUTPUT_DIR}/predictions.tok\")\n",
        "\n",
        "# Detokenize the predictions for evaluation\n",
        "print(\"\\nDetokenizing predictions...\")\n",
        "tok_detok.detokenize(\n",
        "    f'{OUTPUT_DIR}/predictions.tok',\n",
        "    f'{OUTPUT_DIR}/predictions.txt'\n",
        ")\n",
        "print(f\"Detokenized translations saved to: {OUTPUT_DIR}/predictions.txt\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare translations with references\n",
        "# Using ORIGINAL (non-tokenized) source and reference for readability\n",
        "# Using DETOKENIZED predictions\n",
        "print(\"=\" * 70)\n",
        "print(\"TRANSLATION EXAMPLES\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Load original source, reference, and detokenized predictions\n",
        "with open(f'{OPENNMT_DATA_PATH}/src-test.txt', 'r') as f:\n",
        "    sources = f.readlines()\n",
        "with open(f'{OPENNMT_DATA_PATH}/tgt-test.txt', 'r') as f:\n",
        "    references = f.readlines()\n",
        "with open(f'{OUTPUT_DIR}/predictions.txt', 'r') as f:  # Detokenized\n",
        "    predictions = f.readlines()\n",
        "\n",
        "# Show first 10 examples\n",
        "for i in range(min(10, len(sources))):\n",
        "    print(f\"\\n--- Example {i+1} ---\")\n",
        "    print(f\"Source (EN):     {sources[i].strip()}\")\n",
        "    print(f\"Reference (FR):  {references[i].strip()}\")\n",
        "    print(f\"Prediction (FR): {predictions[i].strip()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Evaluate with BLEU Score\n",
        "\n",
        "BLEU (Bilingual Evaluation Understudy) is the standard automatic metric for machine translation. It measures n-gram overlap between predictions and references.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate with sacrebleu\n",
        "# Using DETOKENIZED predictions vs ORIGINAL references\n",
        "try:\n",
        "    import sacrebleu\n",
        "    \n",
        "    # Load detokenized predictions and original references\n",
        "    with open(f'{OUTPUT_DIR}/predictions.txt', 'r') as f:  # Detokenized\n",
        "        predictions = [line.strip() for line in f]\n",
        "    with open(f'{OPENNMT_DATA_PATH}/tgt-test.txt', 'r') as f:  # Original\n",
        "        references = [line.strip() for line in f]\n",
        "    \n",
        "    # Compute BLEU\n",
        "    bleu = sacrebleu.corpus_bleu(predictions, [references])\n",
        "    \n",
        "    print(\"=\" * 60)\n",
        "    print(\"EVALUATION RESULTS\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"\\nBLEU Score: {bleu.score:.2f}\")\n",
        "    print(f\"\\nDetailed: {bleu}\")\n",
        "    \n",
        "except ImportError:\n",
        "    print(\"sacrebleu not installed. Install with: pip install sacrebleu\")\n",
        "    print(\"\\nAlternatively, you can evaluate from command line:\")\n",
        "    print(f\"  sacrebleu {OPENNMT_DATA_PATH}/tgt-test.txt < {OUTPUT_DIR}/predictions.txt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Translate Custom Sentences\n",
        "\n",
        "Let's try translating some custom sentences!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Custom sentences to translate\n",
        "custom_sentences = [\n",
        "    \"The weather is beautiful today.\",\n",
        "    \"I love learning new languages.\",\n",
        "    \"Machine translation has improved significantly in recent years.\",\n",
        "    \"Can you help me find the train station?\",\n",
        "    \"This is a test sentence for our translation model.\",\n",
        "]\n",
        "\n",
        "# Write to temp file and tokenize\n",
        "custom_input_path = f'{OUTPUT_DIR}/custom_input.txt'\n",
        "custom_input_tok_path = f'{OUTPUT_DIR}/custom_input.tok'\n",
        "custom_output_tok_path = f'{OUTPUT_DIR}/custom_output.tok'\n",
        "custom_output_path = f'{OUTPUT_DIR}/custom_output.txt'\n",
        "\n",
        "with open(custom_input_path, 'w') as f:\n",
        "    f.write('\\n'.join(custom_sentences) + '\\n')\n",
        "\n",
        "# Tokenize input\n",
        "tok_detok.tokenize(custom_input_path, custom_input_tok_path)\n",
        "\n",
        "# Translate (tokenized input -> tokenized output)\n",
        "result = subprocess.run(\n",
        "    [\n",
        "        'onmt_translate',\n",
        "        '-model', BEST_MODEL,\n",
        "        '-src', custom_input_tok_path,\n",
        "        '-output', custom_output_tok_path,\n",
        "        '-gpu', '0' if torch.cuda.is_available() else '-1',\n",
        "        '-beam_size', '5',\n",
        "    ],\n",
        "    capture_output=True,\n",
        "    text=True\n",
        ")\n",
        "\n",
        "# Detokenize output\n",
        "tok_detok.detokenize(custom_output_tok_path, custom_output_path)\n",
        "\n",
        "# Read detokenized translations\n",
        "with open(custom_output_path, 'r') as f:\n",
        "    translations = f.readlines()\n",
        "\n",
        "# Display results\n",
        "print(\"=\" * 70)\n",
        "print(\"CUSTOM TRANSLATIONS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for en, fr in zip(custom_sentences, translations):\n",
        "    print(f\"\\nEN: {en}\")\n",
        "    print(f\"FR: {fr.strip()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Appendix: Full Training Configuration Reference\n",
        "\n",
        "Here's a more complete configuration for production-quality models:\n",
        "\n",
        "```yaml\n",
        "# Full production configuration example\n",
        "data:\n",
        "  corpus_1:\n",
        "    path_src: data/src-train.txt\n",
        "    path_tgt: data/tgt-train.txt\n",
        "    transforms: [sentencepiece, filtertoolong]\n",
        "    weight: 1\n",
        "  valid:\n",
        "    path_src: data/src-val.txt\n",
        "    path_tgt: data/tgt-val.txt\n",
        "    transforms: [sentencepiece]\n",
        "\n",
        "# Subword tokenization\n",
        "src_subword_model: sentencepiece.model\n",
        "tgt_subword_model: sentencepiece.model\n",
        "src_subword_type: sentencepiece\n",
        "tgt_subword_type: sentencepiece\n",
        "\n",
        "# Filter long sentences\n",
        "src_seq_length: 200\n",
        "tgt_seq_length: 200\n",
        "\n",
        "# Vocabulary\n",
        "src_vocab: vocab.src\n",
        "tgt_vocab: vocab.tgt\n",
        "share_vocab: true\n",
        "\n",
        "# Model (standard Transformer)\n",
        "encoder_type: transformer\n",
        "decoder_type: transformer\n",
        "enc_layers: 6\n",
        "dec_layers: 6\n",
        "heads: 8\n",
        "hidden_size: 512\n",
        "word_vec_size: 512\n",
        "transformer_ff: 2048\n",
        "dropout: [0.1]\n",
        "attention_dropout: [0.1]\n",
        "position_encoding: true\n",
        "\n",
        "# Training\n",
        "train_steps: 100000\n",
        "valid_steps: 5000\n",
        "batch_type: tokens\n",
        "batch_size: 8192\n",
        "accum_count: [2]\n",
        "\n",
        "# Optimization\n",
        "optim: adam\n",
        "learning_rate: 2.0\n",
        "warmup_steps: 8000\n",
        "decay_method: noam\n",
        "adam_beta2: 0.998\n",
        "max_grad_norm: 0\n",
        "label_smoothing: 0.1\n",
        "\n",
        "# Saving\n",
        "save_model: models/model\n",
        "save_checkpoint_steps: 5000\n",
        "keep_checkpoint: 5\n",
        "\n",
        "# Hardware\n",
        "world_size: 1\n",
        "gpu_ranks: [0]\n",
        "```\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
