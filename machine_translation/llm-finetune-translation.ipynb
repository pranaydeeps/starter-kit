{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'\\nCreated: January 2026\\nAuthor: Thomas Moerman\\nDescription: Notebook for fine-tuning an LLM (Mistral) for machine translation using QLoRA.\\n'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created: January 2026\n",
        "Author: LT3\n",
        "Description: Notebook for fine-tuning an LLM (Mistral) for machine translation using QLoRA.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fine-tuning an LLM for Machine Translation\n",
        "\n",
        "This notebook walks you through **fine-tuning a pretrained LLM** (Mistral-7B-Instruct) for machine translation using the ðŸ¤— Hugging Face ecosystem and parameter-efficient fine-tuning (PEFT).\n",
        "\n",
        "## What you'll learn\n",
        "- How to download and prepare parallel translation data\n",
        "- How to format data for instruction-tuned models using chat templates\n",
        "- How to use **QLoRA** (Quantized Low-Rank Adaptation) for efficient fine-tuning\n",
        "- How to train with the `SFTTrainer` from TRL\n",
        "- How to run inference with your fine-tuned translation model\n",
        "\n",
        "## Why QLoRA?\n",
        "Fine-tuning a 7B parameter model would normally require significant GPU memory. QLoRA combines:\n",
        "- **4-bit quantization**: Reduces memory footprint dramatically\n",
        "- **LoRA adapters**: Only trains a small number of additional parameters\n",
        "\n",
        "This allows fine-tuning large models on consumer GPUs!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Imports + Environment Check\n",
        "\n",
        "We use:\n",
        "- **transformers**: models, tokenizers, and training utilities\n",
        "- **datasets**: for loading and processing data\n",
        "- **peft**: Parameter-Efficient Fine-Tuning (LoRA)\n",
        "- **trl**: Transformer Reinforcement Learning (SFTTrainer)\n",
        "- **bitsandbytes**: 4-bit quantization\n",
        "\n",
        "We'll also set a random seed for reproducibility.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch: 2.9.1+cu128\n",
            "CUDA available: True\n",
            "GPU: Tesla V100-SXM2-16GB\n",
            "GPU Memory: 15.8 GB\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "import pandas as pd\n",
        "from datasets import load_dataset, Dataset\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    set_seed,\n",
        ")\n",
        "\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training,\n",
        "    PeftModel,\n",
        "    PeftConfig,\n",
        ")\n",
        "\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "print('PyTorch:', torch.__version__)\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU:', torch.cuda.get_device_name(0))\n",
        "    print('GPU Memory:', round(torch.cuda.get_device_properties(0).total_memory / 1024**3, 1), 'GB')\n",
        "\n",
        "set_seed(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Download the Translation Data\n",
        "\n",
        "We'll use the English-French translation dataset from Hugging Face.\n",
        "\n",
        "The `download_data.py` script downloads the data and saves it as text files:\n",
        "- `en_train.txt` / `fr_train.txt`: Training parallel sentences\n",
        "- `en_validation.txt` / `fr_validation.txt`: Validation parallel sentences\n",
        "- `en_test.txt` / `fr_test.txt`: Test parallel sentences\n",
        "\n",
        "You can run this from the command line:\n",
        "```bash\n",
        "python download_data.py --repo_name LT3/nfr_bt_nmt_english-french --base_path data/en-fr\n",
        "```\n",
        "\n",
        "Or run it directly in the notebook:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data already exists at data/en-fr\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "REPO_NAME = \"LT3/nfr_bt_nmt_english-french\"\n",
        "DATA_PATH = \"data/en-fr\"\n",
        "\n",
        "# Download the data (run once)\n",
        "from download_data import download_and_save_dataset\n",
        "\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    dataset_paths = download_and_save_dataset(REPO_NAME, DATA_PATH)\n",
        "else:\n",
        "    print(f\"Data already exists at {DATA_PATH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Load and Prepare the Data\n",
        "\n",
        "We'll load the parallel text files and create training examples.\n",
        "\n",
        "For **instruction-tuned models**, we need to format the data as a conversation:\n",
        "- **User**: Provides the source sentence with translation instruction\n",
        "- **Assistant**: Provides the translation\n",
        "\n",
        "For this tutorial, we'll use a small subset:\n",
        "- **Train**: 2,000 examples\n",
        "- **Validation**: 500 examples\n",
        "- **Test**: 10 examples (for quick inference testing)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading training data...\n",
            "Loading validation data...\n",
            "Loading test data...\n",
            "\n",
            "Dataset sizes:\n",
            "  Train: 2000 pairs\n",
            "  Validation: 500 pairs\n",
            "  Test: 10 pairs\n",
            "\n",
            "--- Sample training pairs ---\n",
            "EN: Article 199b is replaced by the following:\n",
            "FR: l'articleÂ 199Â ter est remplacÃ© par le texte suivant:\n",
            "\n",
            "EN: at consular offices:\n",
            "FR: dans les bureaux consulaires:\n",
            "\n",
            "EN: The Portuguese authorities have explained that this public interest mission was entrusted to the private sector in accordance with Decree-Law No 197/99 of 8 June 1999 [7], which transposed into national law European Parliament and Council Directive 97/52/EC of 13 October 1997 amending Directives 92/50/EEC, 93/36/EEC and 93/37/EEC concerning the coordination of procedures for the award of public service contracts, public supply contracts and public works contracts respectively [8].\n",
            "FR: Les autoritÃ©s portugaises ont prÃ©cisÃ© que cette mission dâ€™intÃ©rÃªt public avait Ã©tÃ© attribuÃ©e au secteur privÃ©, dans le respect des prescriptions Ã©tablies par le dÃ©cret loi no 197/1999 du 8 juin 1999 [7], qui est lâ€™instrument national de transposition de la directive 97/52/CE du Parlement europÃ©en et du Conseil du 13 octobre 1997 modifiant les directives 92/50/CEE, 93/36/CEE et 93/37/CEE portant coordination des procÃ©dures de passation des marchÃ©s publics de services, des marchÃ©s publics de fournitures et des marchÃ©s publics de travaux respectivement [8].\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def load_parallel_data(en_path, fr_path, max_samples=None):\n",
        "    \"\"\"Load parallel English-French sentence pairs from text files.\"\"\"\n",
        "    with open(en_path, 'r', encoding='utf-8') as f:\n",
        "        en_sentences = [line.strip() for line in f if line.strip()]\n",
        "    with open(fr_path, 'r', encoding='utf-8') as f:\n",
        "        fr_sentences = [line.strip() for line in f if line.strip()]\n",
        "    \n",
        "    # Ensure same length\n",
        "    min_len = min(len(en_sentences), len(fr_sentences))\n",
        "    en_sentences = en_sentences[:min_len]\n",
        "    fr_sentences = fr_sentences[:min_len]\n",
        "    \n",
        "    if max_samples:\n",
        "        en_sentences = en_sentences[:max_samples]\n",
        "        fr_sentences = fr_sentences[:max_samples]\n",
        "    \n",
        "    return en_sentences, fr_sentences\n",
        "\n",
        "# Load the data\n",
        "print(\"Loading training data...\")\n",
        "en_train, fr_train = load_parallel_data(\n",
        "    f\"{DATA_PATH}/en_train.txt\",\n",
        "    f\"{DATA_PATH}/fr_train.txt\",\n",
        "    max_samples=2000  # Use 2k for tutorial\n",
        ")\n",
        "\n",
        "print(\"Loading validation data...\")\n",
        "en_val, fr_val = load_parallel_data(\n",
        "    f\"{DATA_PATH}/en_validation.txt\",\n",
        "    f\"{DATA_PATH}/fr_validation.txt\",\n",
        "    max_samples=500  # Use 500 for tutorial\n",
        ")\n",
        "\n",
        "print(\"Loading test data...\")\n",
        "en_test, fr_test = load_parallel_data(\n",
        "    f\"{DATA_PATH}/en_test.txt\",\n",
        "    f\"{DATA_PATH}/fr_test.txt\",\n",
        "    max_samples=10  # Use 10 for quick testing\n",
        ")\n",
        "\n",
        "print(f\"\\nDataset sizes:\")\n",
        "print(f\"  Train: {len(en_train)} pairs\")\n",
        "print(f\"  Validation: {len(en_val)} pairs\")\n",
        "print(f\"  Test: {len(en_test)} pairs\")\n",
        "\n",
        "# Preview some examples\n",
        "print(\"\\n--- Sample training pairs ---\")\n",
        "for i in range(3):\n",
        "    print(f\"EN: {en_train[i]}\")\n",
        "    print(f\"FR: {fr_train[i]}\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Load the Model and Tokenizer\n",
        "\n",
        "We'll use **Mistral-7B-Instruct-v0.3**, a powerful instruction-tuned model.\n",
        "\n",
        "### Key configurations:\n",
        "- **4-bit quantization** (NF4): Dramatically reduces memory usage\n",
        "- **Double quantization**: Further memory savings\n",
        "- **bfloat16 compute**: For stable training\n",
        "\n",
        "> **Note**: You may need to accept the model's terms on Hugging Face and set your token.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
        "OUTPUT_DIR = \"../models/mistral-translation-en-fr\"\n",
        "\n",
        "# Optional: Set your Hugging Face token if the model requires authentication\n",
        "# import huggingface_hub\n",
        "# huggingface_hub.login(token=\"your_token_here\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading model: mistralai/Mistral-7B-Instruct-v0.3\n",
            "This may take a few minutes...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8351209f459f4b39abe4812a9f5dc433",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loaded successfully!\n",
            "Model dtype: torch.float16\n"
          ]
        }
      ],
      "source": [
        "# Configure 4-bit quantization\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "print(f\"Loading model: {MODEL_NAME}\")\n",
        "print(\"This may take a few minutes...\")\n",
        "\n",
        "# Load the model with quantization\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config,\n",
        "    use_cache=False,  # Disable cache for training\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "print(f\"Model loaded successfully!\")\n",
        "print(f\"Model dtype: {model.dtype}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizer loaded!\n",
            "Vocab size: 32768\n",
            "BOS token: <s>\n",
            "EOS token: </s>\n",
            "PAD token: </s>\n"
          ]
        }
      ],
      "source": [
        "# Load the tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    add_bos_token=True,\n",
        "    add_eos_token=False,  # SFTTrainer adds EOS when packing=True\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# Set padding token (required for batched training)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"  # Right padding for causal LM\n",
        "\n",
        "print(f\"Tokenizer loaded!\")\n",
        "print(f\"Vocab size: {tokenizer.vocab_size}\")\n",
        "print(f\"BOS token: {tokenizer.bos_token}\")\n",
        "print(f\"EOS token: {tokenizer.eos_token}\")\n",
        "print(f\"PAD token: {tokenizer.pad_token}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Format Data with Chat Template\n",
        "\n",
        "Instruction-tuned models expect data in a specific chat format. We use the tokenizer's `apply_chat_template()` method to format our translation examples correctly.\n",
        "\n",
        "### Training format:\n",
        "```\n",
        "[INST] Translate this sentence from English to French:\n",
        "English: <source sentence>\n",
        "French: [/INST] <target sentence>\n",
        "```\n",
        "\n",
        "### Inference format (no target):\n",
        "```\n",
        "[INST] Translate this sentence from English to French:\n",
        "English: <source sentence>\n",
        "French: [/INST]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating training dataset...\n",
            "Creating validation dataset...\n",
            "\n",
            "Dataset sizes:\n",
            "  Train: 2000\n",
            "  Validation: 500\n"
          ]
        }
      ],
      "source": [
        "def format_translation_example(en_sentence, fr_sentence=None, tokenizer=None, for_training=True):\n",
        "    \"\"\"\n",
        "    Format a translation example using the model's chat template.\n",
        "    \n",
        "    Args:\n",
        "        en_sentence: English source sentence\n",
        "        fr_sentence: French target sentence (None for inference)\n",
        "        tokenizer: The tokenizer with chat template\n",
        "        for_training: If True, include the target translation\n",
        "    \n",
        "    Returns:\n",
        "        Formatted string ready for the model\n",
        "    \"\"\"\n",
        "    # Create the user message with translation instruction\n",
        "    user_message = f\"\"\"Translate this sentence from English to French:\n",
        "English: {en_sentence}\n",
        "French:\"\"\"\n",
        "    \n",
        "    # Build the chat\n",
        "    chat = [{\"role\": \"user\", \"content\": user_message}]\n",
        "    \n",
        "    # Add assistant response for training\n",
        "    if for_training and fr_sentence:\n",
        "        chat.append({\"role\": \"assistant\", \"content\": fr_sentence})\n",
        "    \n",
        "    # Apply the chat template\n",
        "    formatted = tokenizer.apply_chat_template(\n",
        "        chat,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=not for_training  # Add prompt for inference\n",
        "    )\n",
        "    \n",
        "    return formatted\n",
        "\n",
        "\n",
        "def create_dataset(en_sentences, fr_sentences, tokenizer, for_training=True):\n",
        "    \"\"\"\n",
        "    Create a HuggingFace Dataset from parallel sentences.\n",
        "    \"\"\"\n",
        "    formatted_examples = []\n",
        "    \n",
        "    for en, fr in zip(en_sentences, fr_sentences):\n",
        "        formatted = format_translation_example(\n",
        "            en, fr, tokenizer, for_training=for_training\n",
        "        )\n",
        "        formatted_examples.append(formatted)\n",
        "    \n",
        "    return Dataset.from_dict({\"text\": formatted_examples})\n",
        "\n",
        "\n",
        "# Create datasets\n",
        "print(\"Creating training dataset...\")\n",
        "train_dataset = create_dataset(en_train, fr_train, tokenizer, for_training=True)\n",
        "\n",
        "print(\"Creating validation dataset...\")\n",
        "val_dataset = create_dataset(en_val, fr_val, tokenizer, for_training=True)\n",
        "\n",
        "print(f\"\\nDataset sizes:\")\n",
        "print(f\"  Train: {len(train_dataset)}\")\n",
        "print(f\"  Validation: {len(val_dataset)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "FORMATTED TRAINING EXAMPLE:\n",
            "============================================================\n",
            "<s>[INST] Translate this sentence from English to French:\n",
            "English: Article 199b is replaced by the following:\n",
            "French:[/INST] l'articleÂ 199Â ter est remplacÃ© par le texte suivant:</s>\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Preview a formatted training example\n",
        "print(\"=\" * 60)\n",
        "print(\"FORMATTED TRAINING EXAMPLE:\")\n",
        "print(\"=\" * 60)\n",
        "print(train_dataset[0][\"text\"])\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "FORMATTED INFERENCE EXAMPLE:\n",
            "============================================================\n",
            "<s>[INST] Translate this sentence from English to French:\n",
            "English: Carrying out the information procedures laid down under Article 5(4) of the Schengen Borders Code and the consultation procedures laid down under Article 25 of the Schengen Convention, falls within the competences of the authorities responsible for border controls and issuing residence permits or visas.\n",
            "French:[/INST]\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "# Preview how inference prompts look (no target)\n",
        "inference_example = format_translation_example(\n",
        "    en_test[0], None, tokenizer, for_training=False\n",
        ")\n",
        "print(\"=\" * 60)\n",
        "print(\"FORMATTED INFERENCE EXAMPLE:\")\n",
        "print(\"=\" * 60)\n",
        "print(inference_example)\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Configure LoRA (Low-Rank Adaptation)\n",
        "\n",
        "LoRA adds small trainable matrices to the model's attention layers. This allows us to:\n",
        "- Train only ~0.1% of the parameters\n",
        "- Keep the original model weights frozen\n",
        "- Save only the small adapter weights\n",
        "\n",
        "### Key LoRA hyperparameters:\n",
        "- **r (rank)**: Size of the low-rank matrices (higher = more capacity, more memory)\n",
        "- **lora_alpha**: Scaling factor for LoRA weights\n",
        "- **lora_dropout**: Dropout for regularization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LoRA configuration defined.\n",
            "  Rank (r): 64\n",
            "  Alpha: 16\n",
            "  Target modules: {'o_proj', 'v_proj', 'k_proj', 'q_proj'}\n",
            "\n",
            "SFTTrainer will apply LoRA during initialization.\n"
          ]
        }
      ],
      "source": [
        "# Configure LoRA\n",
        "# Note: We only define the config here. SFTTrainer will apply it to the model automatically.\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    r=64,  # Rank of the update matrices\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    # Target the attention layers\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
        ")\n",
        "\n",
        "# Prepare model for k-bit training (enables gradient checkpointing, etc.)\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# IMPORTANT: Don't call get_peft_model() here!\n",
        "# SFTTrainer will apply the peft_config when we pass it to the trainer.\n",
        "# If you manually apply PEFT here AND pass peft_config to SFTTrainer, you'll get an error.\n",
        "\n",
        "print(\"LoRA configuration defined.\")\n",
        "print(f\"  Rank (r): {peft_config.r}\")\n",
        "print(f\"  Alpha: {peft_config.lora_alpha}\")\n",
        "print(f\"  Target modules: {peft_config.target_modules}\")\n",
        "print(\"\\nSFTTrainer will apply LoRA during initialization.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Training Configuration\n",
        "\n",
        "We use the `SFTTrainer` (Supervised Fine-Tuning Trainer) from TRL, which is designed for instruction tuning.\n",
        "\n",
        "### Key training hyperparameters:\n",
        "- **learning_rate**: 2e-4 is a good starting point for LoRA\n",
        "- **batch_size**: Adjust based on your GPU memory\n",
        "- **num_train_epochs**: 1-3 epochs usually sufficient\n",
        "- **max_seq_length**: Maximum sequence length (including prompt + translation)\n",
        "- **packing**: Combines short examples to maximize GPU utilization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training configuration:\n",
            "  Epochs: 1\n",
            "  Batch size: 4\n",
            "  Gradient accumulation: 4\n",
            "  Effective batch size: 16\n",
            "  Learning rate: 0.0002\n",
            "  Max length: 512\n",
            "  Packing: True\n"
          ]
        }
      ],
      "source": [
        "# Maximum sequence length for training\n",
        "MAX_LENGTH = 512  # Increase if your translations are longer\n",
        "\n",
        "# Training configuration using SFTConfig (combines TrainingArguments + SFT-specific settings)\n",
        "sft_config = SFTConfig(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    \n",
        "    # Training hyperparameters\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=4,  # Reduce if OOM\n",
        "    per_device_eval_batch_size=4,\n",
        "    gradient_accumulation_steps=4,  # Effective batch size = 4 * 4 = 16\n",
        "    \n",
        "    # Optimizer settings\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.01,\n",
        "    warmup_steps=50,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    \n",
        "    # Precision\n",
        "    bf16=True,  # Use bfloat16 for stable training\n",
        "    \n",
        "    # Logging and saving\n",
        "    logging_dir=os.path.join(OUTPUT_DIR, \"logs\"),\n",
        "    logging_steps=5,\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    \n",
        "    # Other settings\n",
        "    report_to=\"none\",  # Change to \"wandb\" for experiment tracking\n",
        "    save_total_limit=2,  # Keep only last 2 checkpoints\n",
        "    \n",
        "    # SFT-specific settings\n",
        "    max_length=MAX_LENGTH,  # Maximum length of tokenized sequences\n",
        "    packing=True,  # Pack multiple examples into one sequence for efficiency\n",
        "    dataset_text_field=\"text\",  # Column name containing the text data\n",
        ")\n",
        "\n",
        "print(\"Training configuration:\")\n",
        "print(f\"  Epochs: {sft_config.num_train_epochs}\")\n",
        "print(f\"  Batch size: {sft_config.per_device_train_batch_size}\")\n",
        "print(f\"  Gradient accumulation: {sft_config.gradient_accumulation_steps}\")\n",
        "print(f\"  Effective batch size: {sft_config.per_device_train_batch_size * sft_config.gradient_accumulation_steps}\")\n",
        "print(f\"  Learning rate: {sft_config.learning_rate}\")\n",
        "print(f\"  Max length: {sft_config.max_length}\")\n",
        "print(f\"  Packing: {sft_config.packing}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Padding-free training is enabled, but the attention implementation is not set to a supported flash attention variant. Padding-free training flattens batches into a single sequence, and only the following implementations are known to reliably support this: flash_attention_2, flash_attention_3, kernels-community/flash-attn2, kernels-community/flash-attn3, kernels-community/vllm-flash-attn3. Using other implementations may lead to unexpected behavior. To ensure compatibility, set `attn_implementation` in the model configuration to one of these supported options or verify that your attention mechanism can handle flattened sequences.\n",
            "You are using packing, but the attention implementation is not set to a supported flash attention variant. Packing gathers multiple samples into a single sequence, and only the following implementations are known to reliably support this: flash_attention_2, flash_attention_3, kernels-community/flash-attn2, kernels-community/flash-attn3, kernels-community/vllm-flash-attn3. Using other implementations may lead to cross-contamination between samples. To avoid this, either disable packing by setting `packing=False`, or set `attn_implementation` in the model configuration to one of these supported options.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "615694a2a41741e8867b955296057eb9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Adding EOS to train dataset:   0%|          | 0/2000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0d0006b120cc4dc08677d69375a65d88",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing train dataset:   0%|          | 0/2000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "389103b22d50481baf55943e3aaf58fc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Packing train dataset:   0%|          | 0/2000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d77c0e9e970f462992c1a1335ecb2150",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Adding EOS to eval dataset:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "597b15ffe71e462cadbe92a5d72f83ab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing eval dataset:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9377c6a17128417d9981ed8074e832ac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Packing eval dataset:   0%|          | 0/500 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainer initialized successfully!\n"
          ]
        }
      ],
      "source": [
        "# Initialize the SFTTrainer\n",
        "# Note: In newer TRL versions, max_seq_length, packing, and dataset_text_field\n",
        "# are passed via SFTConfig instead of directly to SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=sft_config,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,\n",
        "    peft_config=peft_config,\n",
        "    processing_class=tokenizer,  # 'tokenizer' is deprecated, use 'processing_class'\n",
        ")\n",
        "\n",
        "print(\"Trainer initialized successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preview of tokenized and packed data:\n",
            "------------------------------------------------------------\n",
            "<s><s>[INST] Translate this sentence from English to French:\n",
            "English: Having regard to Commission Regulation (EC) No 1410/1999 of 29 June 1999 amending Regulation (EC) No 2808/98 laying down detailed rules for the application of the agrimonetary system for the euro in agriculture and amending the definition of certain operative events provided for in regulations (EEC) No 3889/87, (EEC) No 3886/92, (EEC) No 1793/93, (EEC) No 2700/93 and (EC) No 293/98 [4], and in particular Article 2 thereof,\n",
            "French:[/INST] vu le rÃ¨glement (CE) no 1410/1999 de...\n"
          ]
        }
      ],
      "source": [
        "# Preview what the tokenized data looks like (with packing)\n",
        "print(\"Preview of tokenized and packed data:\")\n",
        "print(\"-\" * 60)\n",
        "sample_decoded = tokenizer.decode(trainer.train_dataset[\"input_ids\"][0][:200])\n",
        "print(sample_decoded + \"...\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Train the Model\n",
        "\n",
        "Now let's fine-tune! Training time depends on:\n",
        "- Dataset size\n",
        "- GPU speed\n",
        "- Batch size\n",
        "\n",
        "With 2,000 examples and 1 epoch, this should take ~10-20 minutes on a semi-decent GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n",
            "============================================================\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 9/26 06:18 < 15:18, 0.02 it/s, Epoch 0.31/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Start training!\n",
        "print(\"Starting training...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"Training complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the training logs\n",
        "logs = trainer.state.log_history\n",
        "logs_path = os.path.join(OUTPUT_DIR, \"training_logs.json\")\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "with open(logs_path, \"w\") as f:\n",
        "    json.dump(logs, f, indent=2)\n",
        "\n",
        "print(f\"Training logs saved to: {logs_path}\")\n",
        "\n",
        "# Print final metrics\n",
        "print(\"\\nFinal training metrics:\")\n",
        "for log in logs:\n",
        "    if \"loss\" in log:\n",
        "        print(f\"  Step {log.get('step', 'N/A')}: loss = {log['loss']:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9) Save the Model\n",
        "\n",
        "We save both:\n",
        "- The LoRA adapter weights (small, ~100MB)\n",
        "- The tokenizer\n",
        "\n",
        "The adapter can be loaded on top of the base model for inference.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save the fine-tuned model\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "\n",
        "print(f\"Model saved to: {OUTPUT_DIR}\")\n",
        "print(f\"\\nSaved files:\")\n",
        "for f in os.listdir(OUTPUT_DIR):\n",
        "    size = os.path.getsize(os.path.join(OUTPUT_DIR, f))\n",
        "    print(f\"  {f}: {size / 1024 / 1024:.2f} MB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10) Inference: Generate Translations\n",
        "\n",
        "Now let's test our fine-tuned model! We'll:\n",
        "1. Load the base model\n",
        "2. Load our LoRA adapter\n",
        "3. Generate translations for test sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For inference, we can reload the model fresh\n",
        "# (or continue using the trained model from memory)\n",
        "\n",
        "# Option 1: Use the model from memory (already loaded)\n",
        "inference_model = model\n",
        "inference_tokenizer = tokenizer\n",
        "\n",
        "# Option 2: Load from disk (uncomment if needed)\n",
        "# from peft import PeftModel, PeftConfig\n",
        "# \n",
        "# # Load the PEFT config to get base model name\n",
        "# peft_config = PeftConfig.from_pretrained(OUTPUT_DIR)\n",
        "# \n",
        "# # Load base model\n",
        "# base_model = AutoModelForCausalLM.from_pretrained(\n",
        "#     peft_config.base_model_name_or_path,\n",
        "#     device_map=\"auto\",\n",
        "#     torch_dtype=torch.bfloat16,\n",
        "# )\n",
        "# \n",
        "# # Load the LoRA adapter\n",
        "# inference_model = PeftModel.from_pretrained(base_model, OUTPUT_DIR)\n",
        "# inference_tokenizer = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n",
        "\n",
        "print(\"Model ready for inference!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def translate(english_text, model, tokenizer, max_new_tokens=100):\n",
        "    \"\"\"\n",
        "    Translate English text to French using the fine-tuned model.\n",
        "    \n",
        "    Args:\n",
        "        english_text: The English sentence to translate\n",
        "        model: The fine-tuned model\n",
        "        tokenizer: The tokenizer\n",
        "        max_new_tokens: Maximum tokens to generate\n",
        "    \n",
        "    Returns:\n",
        "        The French translation\n",
        "    \"\"\"\n",
        "    # Format the prompt\n",
        "    prompt = format_translation_example(\n",
        "        english_text, None, tokenizer, for_training=False\n",
        "    )\n",
        "    \n",
        "    # Tokenize\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=True)\n",
        "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "    \n",
        "    # Generate\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,  # Greedy decoding for deterministic output\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "    \n",
        "    # Decode only the generated part\n",
        "    generated_tokens = outputs[0][inputs[\"input_ids\"].shape[1]:]\n",
        "    translation = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "    \n",
        "    return translation.strip()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test on our held-out test examples\n",
        "print(\"=\" * 70)\n",
        "print(\"TRANSLATION RESULTS ON TEST SET\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for i, (en, fr_ref) in enumerate(zip(en_test, fr_test)):\n",
        "    fr_pred = translate(en, inference_model, inference_tokenizer)\n",
        "    \n",
        "    print(f\"\\n--- Example {i+1} ---\")\n",
        "    print(f\"English:    {en}\")\n",
        "    print(f\"Reference:  {fr_ref}\")\n",
        "    print(f\"Predicted:  {fr_pred}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try your own sentences!\n",
        "custom_sentences = [\n",
        "    \"The weather is beautiful today.\",\n",
        "    \"I love learning new languages.\",\n",
        "    \"Machine translation has improved significantly in recent years.\",\n",
        "    \"Can you help me find the train station?\",\n",
        "]\n",
        "\n",
        "print(\"=\" * 70)\n",
        "print(\"CUSTOM TRANSLATIONS\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "for en in custom_sentences:\n",
        "    fr = translate(en, inference_model, inference_tokenizer)\n",
        "    print(f\"\\nEN: {en}\")\n",
        "    print(f\"FR: {fr}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (Optional) Quick BLEU Evaluation\n",
        "\n",
        "Here's how to compute BLEU score on your test set:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Compute BLEU score\n",
        "# Uncomment and run if you have sacrebleu installed\n",
        "# pip install sacrebleu\n",
        "\n",
        "# import sacrebleu\n",
        "# \n",
        "# # Generate translations for test set\n",
        "# predictions = [translate(en, inference_model, inference_tokenizer) for en in en_test]\n",
        "# \n",
        "# # Compute BLEU\n",
        "# bleu = sacrebleu.corpus_bleu(predictions, [fr_test])\n",
        "# print(f\"BLEU score: {bleu.score:.2f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "mt_starter_kit_venv (3.12.3)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
