{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72772ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCreated: 1st January, 2025\\nAuthor: Aaron Maladry\\nLast Modified: November 6, 2025\\nModified by: Pranaydeep Singh\\nDescription: Script for fine-tuning a Llama model with in-context learning.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created: 1st January, 2025\n",
    "Author: Aaron Maladry\n",
    "Last Modified: November 6, 2025\n",
    "Modified by: Pranaydeep Singh\n",
    "Description: Script for fine-tuning a Llama model with in-context learning.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1fd9d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /usr/local/lib/python3.10/site-packages (4.53.2)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/site-packages (4.0.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/site-packages (1.7.0)\n",
      "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/site-packages (1.11.0)\n",
      "Collecting bitsandbytes\n",
      "  Using cached bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting peft\n",
      "  Using cached peft-0.17.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting trl\n",
      "  Downloading trl-0.25.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.10/site-packages (from transformers) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/artemis/Library/Python/3.10/lib/python/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/site-packages (from datasets) (20.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/site-packages (from datasets) (2.3.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: psutil in /Users/artemis/Library/Python/3.10/lib/python/site-packages (from accelerate) (5.9.6)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/site-packages (from accelerate) (2.2.2)\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting huggingface-hub>=0.24.0 (from datasets)\n",
      "  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Using cached tokenizers-0.22.1-cp39-abi3-macosx_10_12_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.14)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/site-packages (from requests->transformers) (2025.7.14)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/artemis/Library/Python/3.10/lib/python/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/artemis/Library/Python/3.10/lib/python/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/site-packages (from sympy->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Using cached bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
      "Using cached peft-0.17.1-py3-none-any.whl (504 kB)\n",
      "Downloading trl-0.25.0-py3-none-any.whl (462 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m462.8/462.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "Using cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "Using cached tokenizers-0.22.1-cp39-abi3-macosx_10_12_x86_64.whl (3.1 MB)\n",
      "Installing collected packages: huggingface-hub, bitsandbytes, tokenizers, transformers, peft, trl\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.33.4\n",
      "    Uninstalling huggingface-hub-0.33.4:\n",
      "      Successfully uninstalled huggingface-hub-0.33.4\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.2\n",
      "    Uninstalling tokenizers-0.21.2:\n",
      "      Successfully uninstalled tokenizers-0.21.2\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.53.2\n",
      "    Uninstalling transformers-4.53.2:\n",
      "      Successfully uninstalled transformers-4.53.2\n",
      "Successfully installed bitsandbytes-0.42.0 huggingface-hub-0.36.0 peft-0.17.1 tokenizers-0.22.1 transformers-4.57.1 trl-0.25.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.10 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#install dependencies\n",
    "!pip install transformers datasets scikit-learn accelerate bitsandbytes peft trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcd53c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/usr/local/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "import transformers\n",
    "import torch\n",
    "import pandas as pd\n",
    "import re\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import datasets\n",
    "import bitsandbytes as bnb\n",
    "import huggingface_hub\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from trl import SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee8d4dc",
   "metadata": {},
   "source": [
    "### Starter Script 2: Fine-tuning a Llama Model with In-context Learning.\n",
    "\n",
    "Fine-tune a Instruction-tuned model (Llama3.2-1B in this notebook) with In-context Learning ie. providing examples of outputs in the prompt.\n",
    "\n",
    "This form of training is not compute intensive compared to SFT and will be much faster, but less performant.\n",
    "\n",
    "Refer to Starter Script 3 for Inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4599c444",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = \"PLACEHOLDER_TOKEN\" #sometimes you need to set your token here since access to some models is restricted\n",
    "huggingface_hub.login(token=access_token) #only needed if you are using private models or pushing to the hub\n",
    "\n",
    "# TODO: set your model here\n",
    "# take care, you need transformers version 4.43, 4.45 is not yet supported, same for ipex-llm==2.1.0b2\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "# removes the repository name (here meta-llama) for saving the output file\n",
    "save_as_name = model_name.split(\"/\")[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "332d0b49",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "The installed version of bitsandbytes (<0.43.1) requires CUDA, but CUDA is not available. You may need to install PyTorch with CUDA support or upgrade bitsandbytes to >=0.43.1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m\n\u001b[1;32m      6\u001b[0m bnb_config \u001b[38;5;241m=\u001b[39m BitsAndBytesConfig(\n\u001b[1;32m      7\u001b[0m         load_in_4bit\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m         bnb_4bit_use_double_quant\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m         bnb_4bit_quant_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnf4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     10\u001b[0m         bnb_4bit_compute_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16,\n\u001b[1;32m     11\u001b[0m     )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# load model and tokenizer with quantization\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m base_model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbnb_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccess_token\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m base_model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     16\u001b[0m base_model \u001b[38;5;241m=\u001b[39m prepare_model_for_kbit_training(base_model)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:604\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mconfig_class \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39msub_configs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext_config\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    603\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[0;32m--> 604\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    610\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/transformers/modeling_utils.py:277\u001b[0m, in \u001b[0;36mrestore_default_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/transformers/modeling_utils.py:4881\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4872\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m transformers_explicit_filename\u001b[38;5;241m.\u001b[39mendswith(\n\u001b[1;32m   4873\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4874\u001b[0m     ) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m transformers_explicit_filename\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.safetensors.index.json\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   4875\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   4876\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe transformers file in the config seems to be incorrect: it is neither a safetensors file \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4877\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(*.safetensors) nor a safetensors index file (*.safetensors.index.json): \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4878\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransformers_explicit_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4879\u001b[0m         )\n\u001b[0;32m-> 4881\u001b[0m hf_quantizer, config, dtype, device_map \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_quantizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muser_agent\u001b[49m\n\u001b[1;32m   4883\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4885\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4886\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   4887\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot combine Quantization and loading a model from a GGUF file, try again by making sure you did not passed a `quantization_config` or that you did not load a quantized model from the Hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4888\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/transformers/quantizers/auto.py:319\u001b[0m, in \u001b[0;36mget_hf_quantizer\u001b[0;34m(config, quantization_config, dtype, from_tf, from_flax, device_map, weights_only, user_agent)\u001b[0m\n\u001b[1;32m    316\u001b[0m     hf_quantizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m     \u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_environment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    326\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_dtype(dtype)\n\u001b[1;32m    327\u001b[0m     device_map \u001b[38;5;241m=\u001b[39m hf_quantizer\u001b[38;5;241m.\u001b[39mupdate_device_map(device_map)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/transformers/quantizers/quantizer_bnb_4bit.py:98\u001b[0m, in \u001b[0;36mBnb4BitHfQuantizer.validate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbitsandbytes\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;241m<\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.43.1\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m---> 98\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     99\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe installed version of bitsandbytes (<0.43.1) requires CUDA, but CUDA is not available. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    100\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou may need to install PyTorch with CUDA support or upgrade bitsandbytes to >=0.43.1.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    101\u001b[0m         )\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mintegrations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m validate_bnb_backend_availability\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_bitsandbytes_multi_backend_available\n",
      "\u001b[0;31mImportError\u001b[0m: The installed version of bitsandbytes (<0.43.1) requires CUDA, but CUDA is not available. You may need to install PyTorch with CUDA support or upgrade bitsandbytes to >=0.43.1."
     ]
    }
   ],
   "source": [
    "#set up quantization config\n",
    "#quantization helps to reduce the memory footprint of the model\n",
    "#making it possible to finetune on smaller GPUs\n",
    "#4-bit quantization is used here\n",
    "#you can also try 8-bit quantization by changing the config below\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=False,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "    )\n",
    "\n",
    "# load model and tokenizer with quantization\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, token=access_token)\n",
    "base_model.config.use_cache = False\n",
    "base_model = prepare_model_for_kbit_training(base_model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=access_token)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# for Llama models the padding side needs to be set to right, check for other models\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa166f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: replace this prompt for your task, this will be asked during inference for each sample\n",
    "# NOTE: here we also have a placeholder for the label, this is only used during training\n",
    "prompt = r\"\\begin[user]Find the topic of this news article:\\n\\n### Text: {PLACEHOLDER_FOR_INPUTTEXT}\\end[user]\\begin[assistant]### Label:{PLACEHOLDER_FOR_LABEL}\\end[assistant]\"\n",
    "# You can modify the prompt as per your requirement, just make sure to keep the {PLACEHOLDER_FOR_INPUTTEXT} and {PLACEHOLDER_FOR_LABEL} intact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa27832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to set up the prompt template with in-context examples\n",
    "# this is used for both training and inference\n",
    "# it's a bit long and dense but should be easy to modify for your own use-case\n",
    "# only thing you need to change is the system prompt for your task\n",
    "# NOTE: you can also implement more sophisticated selection strategies for in-context examples\n",
    "\n",
    "def set_prompt_template(dataframe, example_df, prompt,tokenizer, n_incontext=1, train=True):\n",
    "    \"\"\" \n",
    "    This function prepares the prompt template for inference. You provide the full dataframe and the example dataframe.\n",
    "    \"\"\"\n",
    "    full_explanations = []\n",
    "    # get an example for each unique label\n",
    "    for i, row in dataframe.iterrows():\n",
    "\n",
    "        chat = []\n",
    "\n",
    "        # NOTE: here you can implement more sophisticated selection strategies for in-context examples, such as similarity search, selecting an example for each label or something like contrastive examples.\n",
    "        # isolate n_incontext examples from example_df\n",
    "        # for each sample, new examples are selected\n",
    "        incontext_examples = example_df.sample(n=n_incontext)\n",
    "        \n",
    "        # shuffle the examples\n",
    "        incontext_examples = incontext_examples.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "        # TODO: set up the system prompt, this can include personality description AND general guidelines.\n",
    "        systemprompt =\" You are an expert trained in identifying different news articles topics based on their content.\\n\"\n",
    "        # adding general task description.\n",
    "        systemprompt += \" Your task is to analyze the news headlines and classify them into one of the following categories: World, Sports, Business, Sci/Tech.\\n\"\n",
    "        \n",
    "        # each of the i\n",
    "        for samplenr, sample in incontext_examples.iterrows():\n",
    "            example_input = prompt.replace(\"{PLACEHOLDER_FOR_INPUTTEXT}\" , sample['text'])\\\n",
    "            .replace(\"{PLACEHOLDER_FOR_LABEL}\", sample['task_labels'])\n",
    "            user_text = re.search(r'\\\\begin\\[user\\](.*?)\\\\end\\[user\\]', example_input, re.DOTALL)\n",
    "            system_text = re.search(r'\\\\begin\\[assistant\\](.*?)\\\\end\\[assistant\\]', example_input, re.DOTALL)\n",
    "            if samplenr == 0:\n",
    "                # adds the system prompt for the first example only\n",
    "                chat.append({\"role\": \"user\", \"content\": systemprompt + user_text.group(0).replace(r\"\\begin[user]\", \"\").replace(r\"\\end[user]\", \"\").replace(\"  \", \" \").capitalize()})\n",
    "            else:\n",
    "                # of it's not the first example, the system prompt is not added again, only the \"prompt\" content that is repeated for each sample and example\n",
    "                chat.append({\"role\": \"user\", \"content\": user_text.group(0).replace(r\"\\begin[user]\", \"\").replace(r\"\\end[user]\", \"\").replace(\"  \", \" \").capitalize()})\n",
    "            # for the in-context examples, we also add the assistant response\n",
    "            chat.append({\"role\": \"assistant\", \"content\": system_text.group(0).replace(r\"\\begin[assistant]\", \"\").replace(r\"\\end[assistant]\", \"\").replace(\"  \", \" \").capitalize()},)\n",
    "\n",
    "\n",
    "        # get the actual to classify text and insert it into the prompt\n",
    "        sample_text = prompt.replace(\"{PLACEHOLDER_FOR_INPUTTEXT}\" , row['text']).replace(\"{PLACEHOLDER_FOR_LABEL}\", row['task_labels'])\n",
    "\n",
    "        # get text between \\begin[user] and \\end[user], removing these placeholder tokens\n",
    "        user_text = re.search(r'\\\\begin\\[user\\](.*?)\\\\end\\[user\\]', sample_text, re.DOTALL)\n",
    "        system_text = re.search(r'\\\\begin\\[assistant\\](.*?)\\\\end\\[assistant\\]', sample_text, re.DOTALL)\n",
    "        \n",
    "        chat.append({\"role\": \"user\",\"content\": user_text.group(0).replace(r\"\\begin[user]\", \"\").replace(r\"\\end[user]\", \"\").replace(\"  \",\" \").capitalize()})\n",
    "        if train:\n",
    "            chat.append({\"role\": \"assistant\",\"content\": system_text.group(0).replace(r\"\\begin[assistant]\", \"\").replace(r\"\\end[assistant]\", \"\").replace(\"  \",\" \").capitalize()})\n",
    "        \n",
    "        # this function automatically sets up the chat tokens for instruction tuning based on the model tokenizer.\n",
    "        input_chat = tokenizer.apply_chat_template(chat, tokenize=False)\n",
    "        full_explanations.append(input_chat)\n",
    "    #add to dataframe, overwrite \"text\" column for training\n",
    "    dataframe[\"text\"] = full_explanations\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05089ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the train data, needs two columns: \"text\" and \"labels\" \n",
    "# The labels should be text, otherwise you need to convert the numbers to textual labels.\n",
    "\n",
    "train_data = datasets.load_dataset(\"ag_news\", split=\"train[:1%]\") # using a small subset for demonstration, replace with your dataset\n",
    "\n",
    "# cannot name this labels for training purposes\n",
    "train_data[\"task_labels\"] = train_data[\"labels\"]\n",
    "\n",
    "# set up the prompt template\n",
    "train_data = set_prompt_template(dataframe=train_data, example_df=train_data, prompt=prompt, tokenizer=tokenizer)\n",
    "trainset = datasets.Dataset.from_pandas(train_data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "341f2391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all linear layer names for LoRA adaptation\n",
    "# We only adapt the linear layers in the model, others are frozen\n",
    "# No changes needed here, can be used directly for any model and task\n",
    "\n",
    "def find_all_linear_names(model): # copied from https://github.com/mzbac/llama2-fine-tune/blob/master/utils.py\n",
    "    cls = bnb.nn.Linear4bit\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "    return list(lora_module_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7368277e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up PEFT LoRA for fine-tuning.\n",
    "# Read more about the different parameters here: https://huggingface.co/docs/peft/en/package_reference/lora\n",
    "lora_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    r=32,\n",
    "    target_modules=find_all_linear_names(base_model),\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "#max_seq_length = 1024\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=trainset,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size=1,  # This is actually the global batch size for SPMD.\n",
    "        num_train_epochs=2,\n",
    "        output_dir=f\"./trained_{save_as_name}\",\n",
    "        eval_accumulation_steps=10,\n",
    "        dataloader_drop_last = True,  # Required for SPMD.\n",
    "        hub_private_repo=True,\n",
    "    ),\n",
    "    peft_config=lora_config\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
