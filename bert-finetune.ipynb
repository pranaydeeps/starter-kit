{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54ebd83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nCreated: September 18, 2025\\nLast Modified: November 6, 2025\\nAuthor: Pranaydeep Singh\\nDescription: Script for fine-tuning a fine-tuned BERT model for text classification with inference.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created: September 18, 2025\n",
    "Author: Pranaydeep Singh\n",
    "Last Modified: November 6, 2025\n",
    "Modified by: Pranaydeep Singh\n",
    "Description: Script for fine-tuning a fine-tuned BERT model for text classification with inference.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453069e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#install dependencies\n",
    "!pip install transformers datasets scikit-learn accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c963a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ce1389",
   "metadata": {},
   "source": [
    "### Starter Script 1: Fine-tuning a BERT model for Classification\n",
    "\n",
    "Use a BERT-base-uncased model directly from the HuggingFace Hub to fine-tune for classification on a dataset also hosted directly on the HF Hub.\n",
    "\n",
    "Fine-tuning with almost standard hyper-parameters and a quick eval loop in the end. \n",
    "\n",
    "Please refer to the advanced metrics script to check additional metrics like F1, Precision, Recall, etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26058d34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"bert-base-uncased\" # Model name as in the HuggingFace Hub\n",
    "TOKENIZER_NAME = \"bert-base-uncased\" # Tokenizer name as in the HuggingFace Hub\n",
    "num_labels = 5  # Number of classes for classification\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(TOKENIZER_NAME)\n",
    "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c19a74",
   "metadata": {},
   "source": [
    "Sample data preparation using the datasets library\n",
    "\n",
    "Here we use the \"ag_news\" dataset as an example\n",
    "\n",
    "You can replace it with any text classification dataset of your choice\n",
    "\n",
    "Make sure the dataset has a 'text' field and a 'label' field\n",
    "\n",
    "To use locally available datasets, you can load them accordingly with load_csv or other methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9212ec8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "DATASET_NAME = \"ag_news\" # Example dataset name from the HuggingFace Hub\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(DATASET_NAME, split=\"train[:1%]\") # Using a small subset for demonstration\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "encoded_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"label\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb41a01a",
   "metadata": {},
   "source": [
    "Fine-tuning the model using the Trainer API\n",
    "\n",
    "You can adjust the training arguments as needed\n",
    "\n",
    "For more advanced training, consider using custom training loops or other libraries\n",
    "\n",
    "Refer to the HuggingFace documentation for more details on training, evaluation and the hyperparameters\n",
    "\n",
    "<https://huggingface.co/docs/transformers/en/main_classes/trainer>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4616b83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='225' max='225' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [225/225 11:10, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.646100</td>\n",
       "      <td>0.519238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.360300</td>\n",
       "      <td>0.265013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.199200</td>\n",
       "      <td>0.189239</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=225, training_loss=0.5442601781421238, metrics={'train_runtime': 683.8516, 'train_samples_per_second': 5.264, 'train_steps_per_second': 0.329, 'total_flos': 236806328217600.0, 'train_loss': 0.5442601781421238, 'epoch': 3.0})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "OUTPUT_DIR = \"./results\" # Directory to save model checkpoints and logs\n",
    "\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    eval_strategy=\"epoch\", # Evaluate at the end of each epoch\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16, # Batch size for training, in case of memory issues, reduce this value\n",
    "    per_device_eval_batch_size=16, # Batch size for evaluation, in case of memory issues, reduce this value\n",
    "    num_train_epochs=3, \n",
    "    weight_decay=0.01,\n",
    "    logging_dir=f\"{OUTPUT_DIR}/logs\", # Directory for storing logs\n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=encoded_dataset,\n",
    "    eval_dataset=encoded_dataset,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "#Model checkpoints are saved in the specified output directory after training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04456862",
   "metadata": {},
   "source": [
    "Load saved model for inference or further evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4b7278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 3]\n",
      "['Business', 'Sci/Tech']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = BertForSequenceClassification.from_pretrained(f\"{OUTPUT_DIR}/checkpoint-225\") # Don't forget to change the checkpoint number based on your training\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\") # Tokenizer isn't saved in the output directory, load it from the original source\n",
    "\n",
    "# Function to classify text for a batch of texts\n",
    "# You can modify this function to take input from a file or other sources as needed\n",
    "\n",
    "def classify_texts(texts):\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_classes = torch.argmax(logits, dim=1).tolist()\n",
    "    return predicted_classes\n",
    "\n",
    "sample_texts = [\n",
    "    \"The stock market crashed today due to economic uncertainty.\",\n",
    "    \"The new movie released last week has received rave reviews.\",\n",
    "]\n",
    "predictions = classify_texts(sample_texts)\n",
    "print(predictions)\n",
    "\n",
    "# Map predicted class indices to labels\n",
    "\n",
    "label_map = {0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tech\"} # Example label mapping for AG News dataset\n",
    "predicted_labels = [label_map[pred] for pred in predictions]\n",
    "print(predicted_labels)\n",
    "\n",
    "# Refer to notebook on metrics for calculating additional metrics like F1, Precision, Recall, etc.\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
